diff --ruN a/stablehlo/docs/spec.md b/stablehlo/docs/spec.md
--- stablehlo/docs/spec.md
+++ stablehlo/docs/spec.md
@@ -89,7 +89,7 @@
 
 ```ebnf
 Type         ::= ValueType | NonValueType
-ValueType    ::= TensorType | QuantizedTensorType | TokenType | TupleType
+ValueType    ::= TensorType | QuantizedTensorType | TokenType | TupleType | BufferType
 NonValueType ::= TensorElementType | QuantizedTensorElementType | FunctionType | StringType
 ```
 
@@ -228,6 +228,21 @@
 TupleType ::= 'tuple' '<' TupleElementTypes '>'
 TupleElementTypes ::= [ValueType {',' ValueType}]
 ```
+
+**Buffer types** represent buffers. For example, in XLA, buffers are
+multidimensional arrays with consistent storage. Similar to **tensor types**,
+buffer types have a **shape** and an **element type**, where a shape represents
+non-negative or unknown **dimension sizes** in the ascending order of the
+corresponding **dimensions** (which are also called **axes**) numbered from `0`
+to `R-1`. The number of dimensions `R` is called **rank**. For example,
+`memref<2x3xf32>` is a buffer type with shape `2x3` and element type `f32`. It
+has two dimensions (or, in other words, two axes) - 0th dimension and 1st
+dimension - whose sizes are 2 and 3. Its rank is 2.
+
+Buffers can be allocated using a `custom_call` to `CreateBuffer` or `Pin` and
+deallocated via a `custom_call` to `Unpin`. Only `custom_call` ops can read and
+write the content inside buffers. See [custom_call](#custom_call) for more
+detail.
 
 **Tuple types** represent tuples, i.e. heterogeneous lists. Tuples are a legacy
 feature which only exists for compatibility with HLO. In HLO, tuples are
@@ -2433,20 +2448,62 @@
 
 #### Inputs
 
-| Label | Name                  | Type                                              |
-|-------|-----------------------|---------------------------------------------------|
-| (I1)  | `inputs`              | variadic number of values                         |
-| (I2)  | `call_target_name`    | constant of type `string`                         |
-| (I3)  | `has_side_effect`     | constant of type `i1`                             |
-| (I4)  | `backend_config`      | constant of type `string` or attribute dictionary |
-| (I5)  | `api_version`         | constant of type `si32`                           |
-| (I6)  | `called_computations` | variadic number of constants of type `string`     |
-
-#### Outputs
-
-| Name      | Type                      |
-|-----------|---------------------------|
-| `results` | variadic number of values |
+| Label | Name                     | Type                                                       |
+|-------|--------------------------|------------------------------------------------------------|
+| (I1)  | `inputs`                 | variadic number of values                                  |
+| (I2)  | `call_target_name`       | constant of type `string`                                  |
+| (I3)  | `has_side_effect`        | constant of type `i1`                                      |
+| (I4)  | `backend_config`         | constant of type `string` or attribute dictionary          |
+| (I5)  | `api_version`            | constant of type `si32`                                    |
+| (I6)  | `called_computations`    | variadic number of constants of type `string`              |
+| (I7)  | `output_operand_aliases` | specify the aliasing parts in the outputs and operands     |
+
+#### Outputs
+
+| Name      | Type                     |
+|-----------|--------------------------|
+| `results` | variadic number of values|
+
+### (XLA GPU Support) Special custom_call targets
+
+There are three special `call_target_name` related to `buffer` types:
+`CreateBuffer` creates an uninitialized `buffer`, `Pin` creates an initialized
+`buffer` and `Unpin` deallocates a `buffer` and returns the content of the
+`buffer`.
+
+```mlir
+%uninitialized_buffer = "stablehlo.custom_call"() {
+  call_target_name = "CreateBuffer",
+  api_version = 4 : i32,
+} : () -> memref<4xf64>
+
+%initialized_buffer = "stablehlo.custom_call"(%init_value) {
+  call_target_name = "Pin",
+  api_version = 4 : i32,
+} : (tensor<4xf64>) -> memref<4xf64>
+
+%dealloc_buffer = "stablehlo.custom_call"(%initialized_buffer) {
+  call_target_name = "Unpin",
+  api_version = 4 : i32,
+} : (memref<4xf64>) -> tensor<4xf64>
+
+```
+
+### Alias
+
+Some custom_call ops may require a part in the outputs and a part in the
+operands to share the same memory. This can be expressed via
+`output_operand_aliases`. An alias pair representation consists a list of output
+ tuple indices representing the output part, and an operand_index along with a
+ list of operand tuple indices representing the operand part. The list of output
+  or operand tuple indices is empty if the corresponding type is not a `tuple`
+  type, and can be arbitrarily long for an arbitrarily nested tuple type. This
+  is similar to [the XLA alias representation](https://www.tensorflow.org/xla/aliasing).
+
+The output part and the input part in an alias pair must have the same type. For
+custom_call ops that aren't call to `CreateBuffer`, `Pin` and `Unpin`, a
+`buffer` operand can appear in at most one pair of alias, and a `buffer` output
+must appear in one pair of alias.
 
 #### Examples
 
@@ -2458,6 +2515,16 @@
   api_version = 4 : i32,
   called_computations = [@foo]
 } : (tensor<f64>) -> tensor<f64>
+
+%updated_buffer = "stablehlo.custom_call"(%buffer) {
+  call_target_name = "Update",
+  api_version = 4 : i32,
+  output_operand_aliases = [
+    #stablehlo.output_operand_alias<output_tuple_indices = [],
+      operand_index = 0,
+      operand_tuple_indices = []>]
+} : (memref<4xf64>) -> memref<4xf64>
+
 ```
 
 ### divide
@@ -3780,9 +3847,9 @@
 
 #### Outputs
 
-| Name     | Type               | Constraints |
-|----------|--------------------|-------------|
-| `result` | any supported type | (C2)        |
+| Name     | Typ                    | Constraints |
+|----------|------------------------|-------------|
+| `result` | any value              | (C2)        |
 
 #### Constraints
 
@@ -6583,10 +6650,10 @@
 #### Examples
 
 ```mlir
-// %val0: [1.0, 2.0]
+// %val0: memref[1.0, 2.0]
 // %val1: (3)
-%result = "stablehlo.tuple"(%val0, %val1) : (tensor<2xf32>, tuple<tensor<i32>>) -> tuple<tensor<2xf32>, tuple<tensor<i32>>>
-// %result: ([1.0, 2.0], (3))
+%result = "stablehlo.tuple"(%val0, %val1) : (memref<2xf32>, tuple<tensor<i32>>) -> tuple<memref<2xf32>, tuple<tensor<i32>>>
+// %result: (memref[1.0, 2.0], (3))
 ```
 
 &nbsp;[More Examples](https://github.com/openxla/stablehlo/tree/main/stablehlo/tests/interpret/tuple_and_get_tuple_element.mlir)
@@ -6692,17 +6759,17 @@
 
 #### Inputs
 
-| Label | Name      | Type                                                    | Constraints |
-|-------|-----------|---------------------------------------------------------|-------------|
-| (I1)  | `operand` | variadic number of tensors, quantized tensors or tokens | (C1-C3)     |
-| (I2)  | `cond`    | function                                                | (C1)        |
-| (I3)  | `body`    | function                                                | (C2)        |
-
-#### Outputs
-
-| Name      | Type                                                    | Constraints |
-|-----------|---------------------------------------------------------|-------------|
-| `results` | variadic number of tensors, quantized tensors or tokens | (C3)        |
+| Label | Name      | Type                                    | Constraints |
+|-------|-----------|-----------------------------------------|-------------|
+| (I1)  | `operand` | variadic number of values               | (C1-C3)     |
+| (I2)  | `cond`    | function                                | (C1)        |
+| (I3)  | `body`    | function                                | (C2)        |
+
+#### Outputs
+
+| Name      | Type                                            | Constraints |
+|-----------|-------------------------------------------------|-------------|
+| `results` | variadic number of values                       | (C3)        |
 
 #### Constraints
 
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.cpp
@@ -72,16 +72,16 @@
 
 Value getEmptySparseTensor(OpBuilder &b, Location loc, ShapedType type,
                            ArrayRef<Value> dynSizes) {
-  return b.create<bufferization::AllocTensorOp>(
-      loc, llvm::cast<TensorType>(type), dynSizes,
+  return bufferization::AllocTensorOp::create(
+      b, loc, llvm::cast<TensorType>(type), dynSizes,
       /*copy=*/Value(),
       /*memory_space=*/IntegerAttr());
 }
 
 Value getEmptyTensor(OpBuilder &b, Location loc, ShapedType type,
                      ArrayRef<Value> dynSizes) {
-  return b.create<tensor::EmptyOp>(
-      loc, type.getShape(), type.getElementType(), dynSizes,
+  return tensor::EmptyOp::create(
+      b, loc, type.getShape(), type.getElementType(), dynSizes,
       llvm::cast<RankedTensorType>(type).getEncoding());
 }
 
@@ -103,8 +103,8 @@
     // Construct sizes for the required dimensions.
     for (const auto &en : llvm::enumerate(resultType.getShape())) {
       if (en.value() != ShapedType::kDynamic) continue;
-      sizes.push_back(b.create<tensor::ExtractOp>(
-          loc, reifiedShapes[0],
+      sizes.push_back(tensor::ExtractOp::create(
+          b, loc, reifiedShapes[0],
           ValueRange{b.create<arith::ConstantIndexOp>(loc, en.index())}));
     }
   }
@@ -126,12 +126,12 @@
   if (auto complexType = llvm::dyn_cast<ComplexType>(type.getElementType())) {
     auto zeroElement = builder.getZeroAttr(complexType.getElementType());
     auto zeroAttr = builder.getArrayAttr({zeroElement, zeroElement});
-    zero = builder.create<complex::ConstantOp>(loc, complexType, zeroAttr);
+    zero = complex::ConstantOp::create(builder, loc, complexType, zeroAttr);
   } else {
     auto zeroAttr = builder.getZeroAttr(type.getElementType());
-    zero = builder.create<arith::ConstantOp>(loc, zeroAttr);
+    zero = arith::ConstantOp::create(builder, loc, zeroAttr);
   }
-  return builder.create<linalg::FillOp>(loc, zero, tensor).result();
+  return linalg::FillOp::create(builder, loc, zero, tensor).result();
 }
 
 Value preSparsify(Operation *op, llvm::SmallVector<Value, 2> &values, Type rtp,
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloLegalizeToLinalg.cpp
@@ -93,7 +93,8 @@
 Value extractIndexFromTensor(OpBuilder &builder, Location loc, Value tensor,
                              ShapedType originalType,
                              ArrayRef<Value> tensorIndex = {}) {
-  Value extracted = builder.create<tensor::ExtractOp>(loc, tensor, tensorIndex);
+  Value extracted =
+      tensor::ExtractOp::create(builder, loc, tensor, tensorIndex);
   if (extracted.getType().isIndex()) return extracted;
   return originalType.getElementType().isUnsignedInteger()
              ? builder.createOrFold<arith::IndexCastUIOp>(
@@ -137,14 +138,14 @@
       auto dimIndPos = dimIndIt - lhsLoopVec.begin();
       auto lhsShape = llvm::cast<RankedTensorType>(lhs.getType()).getShape();
       if (lhsShape[dimIndPos] != ShapedType::kDynamic) continue;
-      dimSize = b.create<tensor::DimOp>(loc, lhs, dimIndPos);
+      dimSize = tensor::DimOp::create(b, loc, lhs, dimIndPos);
     } else {
       // query from rhs vars.
       dimIndIt = std::find(rhsLoopVec.begin(), rhsLoopVec.end(), dimInd);
       auto dimIndPos = dimIndIt - rhsLoopVec.begin();
       auto rhsShape = llvm::cast<RankedTensorType>(rhs.getType()).getShape();
       if (rhsShape[dimIndPos] != ShapedType::kDynamic) continue;
-      dimSize = b.create<tensor::DimOp>(loc, rhs, dimIndPos);
+      dimSize = tensor::DimOp::create(b, loc, rhs, dimIndPos);
     }
     dynSizes.push_back(dimSize);
   }
@@ -290,9 +291,9 @@
       maps.push_back(AffineMap::get(nloops, 0, exprs, rewriter.getContext()));
     }
 
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, resultTy ? resultTy : TypeRange{}, adaptor.getOperands(), output,
-        maps, getEinsumLoopsAttrs(inputInd, reductionAxe),
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, resultTy ? resultTy : TypeRange{}, adaptor.getOperands(),
+        output, maps, getEinsumLoopsAttrs(inputInd, reductionAxe),
         [reductionAxe](OpBuilder &b, Location nestedLoc, ValueRange args) {
           Value resultVal =
               b.create<mlir::arith::MulFOp>(nestedLoc, args[0], args[1]);
@@ -404,8 +405,8 @@
 
     int64_t nloops = resultType.getRank();
     Location loc = op.getLoc();
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc,
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc,
         /*resultTensorTypes=*/resultType,
         /*inputs=*/adaptor.getOperands().front(),
         /*outputBuffers=*/
@@ -567,8 +568,8 @@
 
     auto newOperandType =
         RankedTensorType::get(newOperandShape, operandTy.getElementType());
-    operand = rewriter.create<tensor::CollapseShapeOp>(
-        loc, newOperandType, operand, reassociationMap);
+    operand = tensor::CollapseShapeOp::create(rewriter, loc, newOperandType,
+                                              operand, reassociationMap);
   }
   return operand;
 }
@@ -598,8 +599,8 @@
   }
   dimensions = transposedDimensions;
 
-  return rewriter.create<mlir::stablehlo::TransposeOp>(
-      loc,
+  return mlir::stablehlo::TransposeOp::create(
+      rewriter, loc,
       RankedTensorType::get(transposedOperandShape, operandTy.getElementType()),
       operand, rewriter.getDenseI64ArrayAttr(permutation));
 }
@@ -718,7 +719,7 @@
         getNParallelLoopsAttrs(nloops),
         [&](OpBuilder &nestedBuilder, Location /*nested_loc*/,
             ValueRange args) {
-          nestedBuilder.create<linalg::YieldOp>(loc, *args.begin());
+          linalg::YieldOp::create(nestedBuilder, loc, *args.begin());
         },
         linalg::getPrunedAttributeList(op));
     return success();
@@ -791,14 +792,13 @@
         addedDimensions.push_back(dim);
     }
 
-    Value result = rewriter
-                       .create<linalg::BroadcastOp>(
-                           loc, operand, emptyTensor, addedDimensions,
-                           linalg::getPrunedAttributeList(op))
+    Value result = linalg::BroadcastOp::create(
+                       rewriter, loc, operand, emptyTensor, addedDimensions,
+                       linalg::getPrunedAttributeList(op))
                        .getResults()[0];
 
     if (resultTy != broadcastResultTy) {
-      result = rewriter.create<tensor::CastOp>(loc, resultTy, result);
+      result = tensor::CastOp::create(rewriter, loc, resultTy, result);
     }
 
     rewriter.replaceOp(op, result);
@@ -821,7 +821,8 @@
         RankedTensorType::get(updatedOperandShape, operandTy.getElementType());
 
     if (updatedOperandTy != operandTy) {
-      operand = rewriter.create<tensor::CastOp>(loc, updatedOperandTy, operand);
+      operand =
+          tensor::CastOp::create(rewriter, loc, updatedOperandTy, operand);
     }
 
     return operand;
@@ -943,47 +944,49 @@
           if (isExpansion) {
             // Expand a big value into multiple small values with shifts.
             auto iotaIndex =
-                nestedBuilder.create<linalg::IndexOp>(nestedLoc, maxRank - 1);
-            auto iota = nestedBuilder.create<arith::IndexCastOp>(
-                nestedLoc, inIntType, iotaIndex);
-
-            auto width = nestedBuilder.create<arith::ConstantOp>(
-                nestedLoc,
+                linalg::IndexOp::create(nestedBuilder, nestedLoc, maxRank - 1);
+            auto iota = arith::IndexCastOp::create(nestedBuilder, nestedLoc,
+                                                   inIntType, iotaIndex);
+
+            auto width = arith::ConstantOp::create(
+                nestedBuilder, nestedLoc,
                 nestedBuilder.getIntegerAttr(inIntType, outputBitWidth));
             auto shiftWidth =
-                nestedBuilder.create<arith::MulIOp>(nestedLoc, iota, width);
-            Value inputCasted = nestedBuilder.create<arith::BitcastOp>(
-                nestedLoc, inIntType, args.front());
-            Value shifted = nestedBuilder.create<arith::ShRUIOp>(
-                nestedLoc, inputCasted, shiftWidth);
-            innerResult = nestedBuilder.create<arith::TruncIOp>(
-                nestedLoc, outIntType, shifted);
+                arith::MulIOp::create(nestedBuilder, nestedLoc, iota, width);
+            Value inputCasted = arith::BitcastOp::create(
+                nestedBuilder, nestedLoc, inIntType, args.front());
+            Value shifted = arith::ShRUIOp::create(nestedBuilder, nestedLoc,
+                                                   inputCasted, shiftWidth);
+            innerResult = arith::TruncIOp::create(nestedBuilder, nestedLoc,
+                                                  outIntType, shifted);
           } else if (isContraction) {
             // Combine multiple small values into one big value.
             auto iotaIndex =
-                nestedBuilder.create<linalg::IndexOp>(nestedLoc, maxRank - 1);
-            auto iota = nestedBuilder.create<arith::IndexCastOp>(
-                nestedLoc, outIntType, iotaIndex);
-
-            auto width = nestedBuilder.create<arith::ConstantOp>(
-                nestedLoc,
+                linalg::IndexOp::create(nestedBuilder, nestedLoc, maxRank - 1);
+            auto iota = arith::IndexCastOp::create(nestedBuilder, nestedLoc,
+                                                   outIntType, iotaIndex);
+
+            auto width = arith::ConstantOp::create(
+                nestedBuilder, nestedLoc,
                 nestedBuilder.getIntegerAttr(outIntType, inputBitWidth));
             auto shiftWidth =
-                nestedBuilder.create<arith::MulIOp>(nestedLoc, iota, width);
-            Value inputCasted = nestedBuilder.create<arith::BitcastOp>(
-                nestedLoc, inIntType, args.front());
-            Value inputExt = nestedBuilder.create<arith::ExtUIOp>(
-                nestedLoc, outIntType, inputCasted);
-            Value shifted = nestedBuilder.create<arith::ShLIOp>(
-                nestedLoc, inputExt, shiftWidth);
-            Value accumulatorCasted = nestedBuilder.create<arith::BitcastOp>(
-                nestedLoc, outIntType, args.back());
-            innerResult = nestedBuilder.create<arith::OrIOp>(
-                nestedLoc, outIntType, shifted, accumulatorCasted);
+                arith::MulIOp::create(nestedBuilder, nestedLoc, iota, width);
+            Value inputCasted = arith::BitcastOp::create(
+                nestedBuilder, nestedLoc, inIntType, args.front());
+            Value inputExt = arith::ExtUIOp::create(nestedBuilder, nestedLoc,
+                                                    outIntType, inputCasted);
+            Value shifted = arith::ShLIOp::create(nestedBuilder, nestedLoc,
+                                                  inputExt, shiftWidth);
+            Value accumulatorCasted = arith::BitcastOp::create(
+                nestedBuilder, nestedLoc, outIntType, args.back());
+            innerResult =
+                arith::OrIOp::create(nestedBuilder, nestedLoc, outIntType,
+                                     shifted, accumulatorCasted);
           }
-          innerResult = nestedBuilder.create<arith::BitcastOp>(
-              nestedLoc, outputType.getElementType(), innerResult);
-          nestedBuilder.create<linalg::YieldOp>(nestedLoc, innerResult);
+          innerResult = arith::BitcastOp::create(nestedBuilder, nestedLoc,
+                                                 outputType.getElementType(),
+                                                 innerResult);
+          linalg::YieldOp::create(nestedBuilder, nestedLoc, innerResult);
         },
         linalg::getPrunedAttributeList(op));
     return success();
@@ -1000,10 +1003,10 @@
   //   size = ceil((limit - start)/stride)
   static Value computeSize(Location loc, Value start, Value limit, Value stride,
                            ConversionPatternRewriter &b) {
-    Value delta = b.create<arith::SubIOp>(loc, limit, start);
-    Value ret = b.create<arith::CeilDivUIOp>(loc, delta, stride);
+    Value delta = arith::SubIOp::create(b, loc, limit, start);
+    Value ret = arith::CeilDivUIOp::create(b, loc, delta, stride);
     if (ret.getType().isIndex()) return ret;
-    return b.create<arith::IndexCastOp>(loc, b.getIndexType(), ret);
+    return arith::IndexCastOp::create(b, loc, b.getIndexType(), ret);
   }
 
   LogicalResult matchAndRewrite(
@@ -1025,17 +1028,17 @@
 
     auto resultType = llvm::cast<RankedTensorType>(
         this->typeConverter->convertType(realDynamicSliceOp.getType()));
-    Value zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+    Value zero = arith::ConstantIndexOp::create(rewriter, loc, 0);
     SmallVector<OpFoldResult> offsets, sizes, strides;
     SmallVector<Type, 3> clampType(3, arithType);
     for (auto i : llvm::seq<unsigned>(0, argType.getRank())) {
-      Value dim = rewriter.create<arith::ConstantIndexOp>(loc, i);
-      Value start = rewriter.create<tensor::ExtractOp>(
-          loc, adaptor.getStartIndices(), dim);
-      Value limit = rewriter.create<tensor::ExtractOp>(
-          loc, adaptor.getLimitIndices(), dim);
+      Value dim = arith::ConstantIndexOp::create(rewriter, loc, i);
+      Value start = tensor::ExtractOp::create(rewriter, loc,
+                                              adaptor.getStartIndices(), dim);
+      Value limit = tensor::ExtractOp::create(rewriter, loc,
+                                              adaptor.getLimitIndices(), dim);
       Value stride =
-          rewriter.create<tensor::ExtractOp>(loc, adaptor.getStrides(), dim);
+          tensor::ExtractOp::create(rewriter, loc, adaptor.getStrides(), dim);
 
       // Compute i-th dimension size of the result : size[i].
       // If the i-th dimension of the result type is known, we go ahead with it
@@ -1044,12 +1047,12 @@
       Value size =
           ShapedType::isDynamic(resultDimSize)
               ? computeSize(loc, start, limit, stride, rewriter)
-              : rewriter.create<arith::ConstantIndexOp>(loc, resultDimSize);
+              : arith::ConstantIndexOp::create(rewriter, loc, resultDimSize);
 
       // We can now convert start to index.
       if (!start.getType().isIndex())
-        start = rewriter.create<arith::IndexCastOp>(
-            loc, rewriter.getIndexType(), start);
+        start = arith::IndexCastOp::create(rewriter, loc,
+                                           rewriter.getIndexType(), start);
 
       // Fetch i-th dimension size of the operand and calculate upper bound as
       //   ub = operand_dim[i] - size[i]
@@ -1061,8 +1064,8 @@
       // We clamp the start_index to keep it bounded as
       //   0 <= start_index[i] <= ub
       // Clamp does not support index type, so cast to integer type.
-      start = rewriter.create<arith::MaxSIOp>(loc, start, zero);
-      start = rewriter.create<arith::MinSIOp>(loc, start, upperBound);
+      start = arith::MaxSIOp::create(rewriter, loc, start, zero);
+      start = arith::MinSIOp::create(rewriter, loc, start, upperBound);
 
       offsets.push_back(start);
       if (ShapedType::isDynamic(resultDimSize))
@@ -1118,8 +1121,8 @@
       // cast the dynamic dimensions to 1.
       auto staticType = RankedTensorType::get(
           llvm::SmallVector<int64_t>(operandType.getRank(), 1), elemType);
-      operand = rewriter.create<tensor::CastOp>(reshapeOp.getLoc(), staticType,
-                                                operand);
+      operand = tensor::CastOp::create(rewriter, reshapeOp.getLoc(), staticType,
+                                       operand);
       rewriter.replaceOpWithNewOp<tensor::CollapseShapeOp>(
           reshapeOp, resultType, operand, ArrayRef<ReassociationIndices>{});
       return success();
@@ -1153,8 +1156,8 @@
         auto enc = sparse_tensor::getSparseTensorEncoding(operandType);
         auto newOperandType = RankedTensorType::get(shape, elemType, enc);
         if (newOperandType != operandType) {
-          operand = rewriter.create<tensor::CastOp>(reshapeOp.getLoc(),
-                                                    newOperandType, operand);
+          operand = tensor::CastOp::create(rewriter, reshapeOp.getLoc(),
+                                           newOperandType, operand);
         }
         // Generate collapse operation.
         // For scalar collapses must pass an empty reassociation map.
@@ -1188,14 +1191,14 @@
           // dimensions.
           getIdentityExprs(operandType.getRank())};
 
-      collapsedOp =
-          rewriter.create<tensor::CollapseShapeOp>(loc, operand, collapsingMap);
+      collapsedOp = tensor::CollapseShapeOp::create(rewriter, loc, operand,
+                                                    collapsingMap);
     }
     // Cast to a known static type if the input has dynamic dimensions.
     int64_t totalElems = resultType.getNumElements();
     auto collapsedType = RankedTensorType::get({totalElems}, elemType);
     collapsedOp =
-        rewriter.create<tensor::CastOp>(loc, collapsedType, collapsedOp);
+        tensor::CastOp::create(rewriter, loc, collapsedType, collapsedOp);
     if (resultType.getRank() == 1) {
       rewriter.replaceOp(reshapeOp, collapsedOp);
     } else {
@@ -1233,8 +1236,8 @@
     unsigned nloops = resultShapedType.getRank();
 
     Location loc = iotaOp.getLoc();
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc,
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc,
         /*resultTensorTypes=*/
         ArrayRef<Type>{resultShapedType},
         /*inputs=*/ValueRange{},
@@ -1288,8 +1291,8 @@
     Value empty = getEmptyTensorFor(rewriter, loc, resultTy, iotaOp,
                                     adaptor.getOperands());
 
-    auto linalgOp = rewriter.create<linalg::MapOp>(
-        loc, ValueRange{}, empty,
+    auto linalgOp = linalg::MapOp::create(
+        rewriter, loc, ValueRange{}, empty,
         [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange /*args*/) {
           Value index = nestedBuilder.create<linalg::IndexOp>(
               nestedLoc, iotaOp.getIotaDimension());
@@ -1345,11 +1348,11 @@
 
     if (enablePrimitiveOps) {
       auto concatOp =
-          rewriter.create<tensor::ConcatOp>(loc, dim, adaptor.getOperands());
+          tensor::ConcatOp::create(rewriter, loc, dim, adaptor.getOperands());
       rewriter.replaceOp(op, concatOp);
       return success();
     }
-    Value zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+    Value zero = arith::ConstantIndexOp::create(rewriter, loc, 0);
 
     // Allocate the output tensor with tensor.empty.
     Value result =
@@ -1372,25 +1375,26 @@
           SmallVector<Value> extractIndices;
           extractIndices.reserve(nloops);
           for (int64_t i = 0; i < nloops; i++) {
-            extractIndices.push_back(b.create<linalg::IndexOp>(loc, i));
+            extractIndices.push_back(linalg::IndexOp::create(b, loc, i));
           }
 
-          Value indexOp = b.create<linalg::IndexOp>(loc, dim);
+          Value indexOp = linalg::IndexOp::create(b, loc, dim);
           for (auto [idx, arg] : llvm::enumerate(adaptor.getOperands())) {
             Value newConcatDimSize;
             scf::IfOp ifOp;
             if (idx + 1 != adaptor.getOperands().size()) {
               // Calculate how far along we have iterated along the concatenate
               // dimension. That way we can tell which input to select.
-              newConcatDimSize = b.create<arith::AddIOp>(
-                  loc, concatDimSize, b.create<tensor::DimOp>(loc, arg, dim));
-              Value cmp = b.create<arith::CmpIOp>(loc, rewriter.getI1Type(),
-                                                  arith::CmpIPredicate::ult,
-                                                  indexOp, newConcatDimSize);
-              ifOp = b.create<scf::IfOp>(loc, resultType.getElementType(), cmp,
-                                         true);
+              newConcatDimSize =
+                  arith::AddIOp::create(b, loc, concatDimSize,
+                                        b.create<tensor::DimOp>(loc, arg, dim));
+              Value cmp = arith::CmpIOp::create(b, loc, rewriter.getI1Type(),
+                                                arith::CmpIPredicate::ult,
+                                                indexOp, newConcatDimSize);
+              ifOp = scf::IfOp::create(b, loc, resultType.getElementType(), cmp,
+                                       true);
               if (result) {
-                b.create<scf::YieldOp>(loc, ifOp->getResults()[0]);
+                scf::YieldOp::create(b, loc, ifOp->getResults()[0]);
               } else {
                 result = ifOp->getResults()[0];
               }
@@ -1401,17 +1405,17 @@
             // Now adjust the index for the concatenated dimension to fit into
             // the selected tensor and do an extract at that position.
             extractIndices[dim] =
-                b.create<arith::SubIOp>(loc, indexOp, concatDimSize);
+                arith::SubIOp::create(b, loc, indexOp, concatDimSize);
             Value extract =
-                b.create<tensor::ExtractOp>(loc, arg, extractIndices);
-            b.create<scf::YieldOp>(loc, extract);
+                tensor::ExtractOp::create(b, loc, arg, extractIndices);
+            scf::YieldOp::create(b, loc, extract);
 
             if (ifOp) {
               b = ifOp.getElseBodyBuilder(b.getListener());
               concatDimSize = newConcatDimSize;
             }
           }
-          nestedBuilder.create<linalg::YieldOp>(loc, result);
+          linalg::YieldOp::create(nestedBuilder, loc, result);
         },
         linalg::getPrunedAttributeList(op));
     return success();
@@ -1566,15 +1570,15 @@
       Value startIndex =
           extractIndexFromTensor(rewriter, loc, start, originalStartIndexType);
 
-      Value mn = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+      Value mn = arith::ConstantIndexOp::create(rewriter, loc, 0);
 
       Value mx =
           rewriter.createOrFold<tensor::DimOp>(loc, adaptor.getOperand(), idx);
       mx = rewriter.createOrFold<arith::SubIOp>(
-          loc, mx, rewriter.create<arith::ConstantIndexOp>(loc, size));
-
-      startIndex = rewriter.create<arith::MaxSIOp>(loc, startIndex, mn);
-      startIndex = rewriter.create<arith::MinSIOp>(loc, startIndex, mx);
+          loc, mx, arith::ConstantIndexOp::create(rewriter, loc, size));
+
+      startIndex = arith::MaxSIOp::create(rewriter, loc, startIndex, mn);
+      startIndex = arith::MinSIOp::create(rewriter, loc, startIndex, mx);
 
       startIndices.push_back(startIndex);
     }
@@ -1620,7 +1624,7 @@
     }
 
     SmallVector<OpFoldResult, 3> startIndices;
-    Value zero = rewriter.create<arith::ConstantIndexOp>(loc, 0);
+    Value zero = arith::ConstantIndexOp::create(rewriter, loc, 0);
     for (auto [idx, start] : llvm::enumerate(adaptor.getStartIndices())) {
       // By stablehlo.DynamicUpdateSlice definition:
       //   `start_indices[i] = clamp(start_indices[i],
@@ -1628,11 +1632,12 @@
       Value startIndex = extractIndexFromTensor(
           rewriter, loc, start,
           cast<ShapedType>(op.getStartIndices()[idx].getType()));
-      Value ub = rewriter.create<arith::ConstantIndexOp>(
-          loc, operandType.getDimSize(idx) - updateType.getDimSize(idx));
-
-      startIndex = rewriter.create<arith::MaxSIOp>(loc, startIndex, zero);
-      startIndex = rewriter.create<arith::MinSIOp>(loc, startIndex, ub);
+      Value ub = arith::ConstantIndexOp::create(
+          rewriter, loc,
+          operandType.getDimSize(idx) - updateType.getDimSize(idx));
+
+      startIndex = arith::MaxSIOp::create(rewriter, loc, startIndex, zero);
+      startIndex = arith::MinSIOp::create(rewriter, loc, startIndex, ub);
       startIndices.push_back(startIndex);
     }
 
@@ -1667,8 +1672,8 @@
         op.getNumOperands() + 1,
         rewriter.getMultiDimIdentityMap(resultType.getRank()));
 
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, resultType, adaptor.getOperands(), output, indexingMaps,
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, resultType, adaptor.getOperands(), output, indexingMaps,
         getNParallelLoopsAttrs(resultType.getRank()),
         /*bodyBuild=*/nullptr, linalg::getPrunedAttributeList(op));
 
@@ -1718,12 +1723,12 @@
           rewriter, loc, cast<TypedValue<ShapedType>>(operand),
           cast<ShapedType>(operand0.getType())));
     }
-    Value output = rewriter.create<tensor::EmptyOp>(
-        loc, tensor::getMixedSizes(rewriter, loc, operand0),
+    Value output = tensor::EmptyOp::create(
+        rewriter, loc, tensor::getMixedSizes(rewriter, loc, operand0),
         resultType.getElementType());
 
-    auto linalgOp = rewriter.create<linalg::MapOp>(
-        loc, coercedOperands, output,
+    auto linalgOp = linalg::MapOp::create(
+        rewriter, loc, coercedOperands, output,
         /*bodyBuild=*/nullptr, linalg::getPrunedAttributeList(op));
 
     // Convert the signature of the body. We scalarize the operands and add a
@@ -1798,7 +1803,7 @@
     for (int64_t i = 0, e = std::max({resultRank, operandRank, int64_t{2}});
          i < e; ++i) {
       constants.push_back(
-          rewriter.create<arith::ConstantOp>(loc, rewriter.getIndexAttr(i)));
+          arith::ConstantOp::create(rewriter, loc, rewriter.getIndexAttr(i)));
     }
 
     Value emptyOp = getEmptyTensorFor(rewriter, loc, resultType, gatherOp,
@@ -1807,8 +1812,8 @@
     ValueRange ins;
     SmallVector<AffineMap, 1> indexingMaps(
         {rewriter.getMultiDimIdentityMap(resultRank)});
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, /*resultTensorTypes=*/resultType,
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, /*resultTensorTypes=*/resultType,
         /*inputs=*/ins,
         /*outputs=*/emptyOp, indexingMaps, getNParallelLoopsAttrs(resultRank),
         /*bodyBuild=*/nullptr, linalg::getPrunedAttributeList(gatherOp));
@@ -1832,7 +1837,7 @@
     // potentially getting duplicates later.
     SmallVector<Value> linalgIndices;
     for (int64_t i = 0; i < resultRank; ++i) {
-      linalgIndices.push_back(rewriter.create<linalg::IndexOp>(loc, i));
+      linalgIndices.push_back(linalg::IndexOp::create(rewriter, loc, i));
     }
 
     // Now the complicated part. For a given output dimension we build up an
@@ -1921,8 +1926,8 @@
           loc, operandDimSize, outputDimSize);
 
       // Clamp indices to [0, i, operand_dim-output_dim].
-      Value clamp = rewriter.create<arith::MinSIOp>(
-          loc,
+      Value clamp = arith::MinSIOp::create(
+          rewriter, loc,
           rewriter.create<arith::MaxSIOp>(loc, constants[0],
                                           remappedIndexFromIndices[i]),
           largestValidIndex);
@@ -1957,11 +1962,11 @@
       SmallVector<int64_t> dims(operandRank, ShapedType::kDynamic);
       auto type = RankedTensorType::get(
           dims, cast<TensorType>(operand.getType()).getElementType());
-      extractOperand = rewriter.create<tensor::CastOp>(loc, type, operand);
+      extractOperand = tensor::CastOp::create(rewriter, loc, type, operand);
     }
     Value element =
-        rewriter.create<tensor::ExtractOp>(loc, extractOperand, combinedIndex);
-    rewriter.create<linalg::YieldOp>(loc, element);
+        tensor::ExtractOp::create(rewriter, loc, extractOperand, combinedIndex);
+    linalg::YieldOp::create(rewriter, loc, element);
 
     rewriter.replaceOp(gatherOp, linalgOp.getResults());
 
@@ -2083,24 +2088,25 @@
     SmallVector<Value> reduceDynSizes;
     for (int i = 0, s = rank; i < s; ++i)
       if (sourceTy.isDynamicDim(i))
-        reduceDynSizes.push_back(b.create<tensor::DimOp>(source, i));
-
-    Value reduceValueEmpty =
-        b.create<tensor::EmptyOp>(sourceTy.getShape(), destETy, reduceDynSizes);
-    Value reduceIndexEmpty = b.create<tensor::EmptyOp>(
-        sourceTy.getShape(), indexETy, reduceDynSizes);
+        reduceDynSizes.push_back(tensor::DimOp::create(b, source, i));
+
+    Value reduceValueEmpty = tensor::EmptyOp::create(b, sourceTy.getShape(),
+                                                     destETy, reduceDynSizes);
+    Value reduceIndexEmpty = tensor::EmptyOp::create(b, sourceTy.getShape(),
+                                                     indexETy, reduceDynSizes);
 
     // We initialize indices to -1 which indicates no matching destination.
-    Value negativeOne = b.create<arith::ConstantOp>(b.getI32IntegerAttr(-1));
+    Value negativeOne = arith::ConstantOp::create(b, b.getI32IntegerAttr(-1));
     reduceIndexEmpty =
-        b.create<linalg::FillOp>(negativeOne, reduceIndexEmpty).getResult(0);
+        linalg::FillOp::create(b, negativeOne, reduceIndexEmpty).getResult(0);
 
     // We only care to match the reduction dimensions.
-    Value windowEmpty = b.create<tensor::EmptyOp>(filteredWindows, srcETy);
-
-    auto reduceGeneric = b.create<linalg::GenericOp>(
-        /*resultTensors=*/ArrayRef<Type>{reduceValueEmpty.getType(),
-                                         reduceIndexEmpty.getType()},
+    Value windowEmpty = tensor::EmptyOp::create(b, filteredWindows, srcETy);
+
+    auto reduceGeneric = linalg::GenericOp::create(
+        b,
+        /*resultTensors=*/
+        ArrayRef<Type>{reduceValueEmpty.getType(), reduceIndexEmpty.getType()},
         /*inputs=*/ValueRange{operand, windowEmpty},
         /*outputs=*/ValueRange{reduceValueEmpty, reduceIndexEmpty},
         reduceIndexingMaps,
@@ -2137,32 +2143,32 @@
     // The predicate operates on scalar-tensors, so we need to extract the
     // value for `linalg` operations. Tensor-ops are cleaned up by other
     // rewriters.
-    selectPred = b.create<tensor::ExtractOp>(rewriter.getI1Type(), selectPred,
-                                             ValueRange{});
+    selectPred = tensor::ExtractOp::create(b, rewriter.getI1Type(), selectPred,
+                                           ValueRange{});
 
     // We select if either the selection function returns `true` or the
     // current reduction index is `-1`, e.g. no index has been selected yet.
-    Value selectNegOne = b.create<arith::CmpIOp>(arith::CmpIPredicate::eq,
-                                                 selectOutIdx, negativeOne);
-    selectPred = b.create<arith::OrIOp>(selectPred, selectNegOne);
+    Value selectNegOne = arith::CmpIOp::create(b, arith::CmpIPredicate::eq,
+                                               selectOutIdx, negativeOne);
+    selectPred = arith::OrIOp::create(b, selectPred, selectNegOne);
 
     // We compute a unique idx for each element in the window.
-    Value computedIdx = b.create<linalg::IndexOp>(rank);
+    Value computedIdx = linalg::IndexOp::create(b, rank);
     for (int i = 1, s = filteredStrides.size(); i < s; ++i) {
-      Value width = b.create<arith::ConstantIndexOp>(filteredStrides[i]);
-      Value idx = b.create<linalg::IndexOp>(rank + i);
-      computedIdx = b.create<arith::MulIOp>(width, computedIdx);
-      computedIdx = b.create<arith::AddIOp>(computedIdx, idx);
-    }
-    computedIdx = b.create<arith::IndexCastOp>(indexETy, computedIdx);
+      Value width = arith::ConstantIndexOp::create(b, filteredStrides[i]);
+      Value idx = linalg::IndexOp::create(b, rank + i);
+      computedIdx = arith::MulIOp::create(b, width, computedIdx);
+      computedIdx = arith::AddIOp::create(b, computedIdx, idx);
+    }
+    computedIdx = arith::IndexCastOp::create(b, indexETy, computedIdx);
 
     // Using the selection predicate track the value and selected
     // identifier for the future scattering.
     Value selectedIdx =
-        b.create<arith::SelectOp>(selectPred, computedIdx, selectOutIdx);
+        arith::SelectOp::create(b, selectPred, computedIdx, selectOutIdx);
     Value selectedValue =
-        b.create<arith::SelectOp>(selectPred, selectInVal, selectOutVal);
-    b.create<linalg::YieldOp>(ValueRange{selectedValue, selectedIdx});
+        arith::SelectOp::create(b, selectPred, selectInVal, selectOutVal);
+    linalg::YieldOp::create(b, ValueRange{selectedValue, selectedIdx});
 
     // Original terminator is an stablehlo.return we no longer need.
     rewriter.eraseOp(reduceTerminator);
@@ -2181,7 +2187,7 @@
     for (int i = 0, s = reduceIndexTy.getRank(); i < s; ++i) {
       int64_t broadcast = strides[i];
       if (sourceTy.isDynamicDim(i))
-        broadcastDynDims.push_back(b.create<tensor::DimOp>(source, i));
+        broadcastDynDims.push_back(tensor::DimOp::create(b, source, i));
 
       broadcastExprs.push_back(b.getAffineDimExpr(broadcastShape.size()));
       broadcastShape.push_back(sourceTy.getDimSize(i));
@@ -2192,12 +2198,12 @@
 
     // We broadcast the values of our input tensors across the stride-tiling
     // size.
-    Value scatterEmpty = b.create<tensor::EmptyOp>(
-        broadcastShape, resultTy.getElementType(), broadcastDynDims);
-    Value initScalar = b.create<tensor::ExtractOp>(initTy.getElementType(),
-                                                   init, ValueRange{});
+    Value scatterEmpty = tensor::EmptyOp::create(
+        b, broadcastShape, resultTy.getElementType(), broadcastDynDims);
+    Value initScalar = tensor::ExtractOp::create(b, initTy.getElementType(),
+                                                 init, ValueRange{});
     Value scatterFill =
-        b.create<linalg::FillOp>(initScalar, scatterEmpty).getResult(0);
+        linalg::FillOp::create(b, initScalar, scatterEmpty).getResult(0);
 
     // Both the indices and values are broadcasted using the same indexing map.
     // Output fully parallel.
@@ -2210,7 +2216,8 @@
     scatterIndexingMaps.push_back(
         b.getMultiDimIdentityMap(broadcastShape.size()));
 
-    auto scatterGeneric = b.create<linalg::GenericOp>(
+    auto scatterGeneric = linalg::GenericOp::create(
+        b,
         /*resultTensors=*/ArrayRef<Type>{scatterFill.getType()},
         /*inputs=*/ValueRange{reduceIndex, source},
         /*outputs=*/ValueRange{scatterFill}, scatterIndexingMaps,
@@ -2237,32 +2244,32 @@
 
     Value scatterInputIdx = scatterBlock.getArgument(0);
     Value scatterOutputVal = scatterBlock.getArgument(2);
-    Value scatterUpdate = b.create<tensor::ExtractOp>(
-        sourceTy.getElementType(), scatterTerminator->getOperand(0),
+    Value scatterUpdate = tensor::ExtractOp::create(
+        b, sourceTy.getElementType(), scatterTerminator->getOperand(0),
         ValueRange{});
 
     // Compute the index of the tiled region to determine if it was selected.
-    Value id = b.create<arith::ConstantIndexOp>(0);
+    Value id = arith::ConstantIndexOp::create(b, 0);
     int64_t dim = 0;
     for (int i = 0, s = strides.size(); i < s; ++i) {
       if (strides[i] > 1) {
-        Value idx = b.create<linalg::IndexOp>(++dim);
-        Value tileSz = b.create<arith::ConstantIndexOp>(strides[i]);
-        id = b.create<arith::MulIOp>(id, tileSz);
-        id = b.create<arith::AddIOp>(id, idx);
+        Value idx = linalg::IndexOp::create(b, ++dim);
+        Value tileSz = arith::ConstantIndexOp::create(b, strides[i]);
+        id = arith::MulIOp::create(b, id, tileSz);
+        id = arith::AddIOp::create(b, id, idx);
       }
       ++dim;
     }
 
     // Check whether the computed id matches the to-scatter id, then select and
     // yield.
-    id = b.create<arith::IndexCastOp>(indexETy, id);
-    auto scatterPred = b.create<arith::CmpIOp>(
-        b.getI1Type(), arith::CmpIPredicate::eq, id, scatterInputIdx);
-    scatterUpdate =
-        b.create<arith::SelectOp>(scatterPred, scatterUpdate, scatterOutputVal);
-
-    b.create<linalg::YieldOp>(scatterUpdate);
+    id = arith::IndexCastOp::create(b, indexETy, id);
+    auto scatterPred = arith::CmpIOp::create(
+        b, b.getI1Type(), arith::CmpIPredicate::eq, id, scatterInputIdx);
+    scatterUpdate = arith::SelectOp::create(b, scatterPred, scatterUpdate,
+                                            scatterOutputVal);
+
+    linalg::YieldOp::create(b, scatterUpdate);
     rewriter.eraseOp(scatterTerminator);
     b.setInsertionPoint(op);
 
@@ -2278,8 +2285,8 @@
       collapseDim += dims.size();
     }
 
-    Value collapse = b.create<tensor::CollapseShapeOp>(
-        scatterGeneric.getResult(0), reassociationMap);
+    Value collapse = tensor::CollapseShapeOp::create(
+        b, scatterGeneric.getResult(0), reassociationMap);
     auto collapseTy = llvm::cast<ShapedType>(collapse.getType());
 
     // After collapsing it it possible that the target may need to be padded.
@@ -2294,15 +2301,15 @@
         size = ShapedType::kDynamic;
       padShape.push_back(size);
 
-      Value in = b.create<tensor::DimOp>(collapse, i);
-      Value out = b.create<tensor::DimOp>(operand, i);
-      Value diff = b.create<arith::SubIOp>(out, in);
+      Value in = tensor::DimOp::create(b, collapse, i);
+      Value out = tensor::DimOp::create(b, operand, i);
+      Value diff = arith::SubIOp::create(b, out, in);
       Value pad = b.createOrFold<arith::MaxSIOp>(diff, zero);
       padHigh.push_back(pad);
     }
 
-    Value padded = b.create<tensor::PadOp>(collapseTy.clone(padShape), collapse,
-                                           padLow, padHigh, initScalar);
+    Value padded = tensor::PadOp::create(b, collapseTy.clone(padShape),
+                                         collapse, padLow, padHigh, initScalar);
 
     // The result may exceed the target size, slice if necessary.
     SmallVector<OpFoldResult> sliceSizes;
@@ -2363,9 +2370,9 @@
     if (!hasNegativePadding) return failure();
 
     // Create a new pad op with the positive values.
-    Value pad = rewriter.create<mlir::stablehlo::PadOp>(
-        op.getLoc(), adaptor.getOperand(), adaptor.getPaddingValue(), padLow,
-        padHigh, op.getInteriorPadding());
+    Value pad = mlir::stablehlo::PadOp::create(
+        rewriter, op.getLoc(), adaptor.getOperand(), adaptor.getPaddingValue(),
+        padLow, padHigh, op.getInteriorPadding());
 
     // Then slice according to the negative edge padding. Static shapes only for
     // now.
@@ -2410,8 +2417,8 @@
     // If there is no interior padding lower to tensor.pad directly.
     if (llvm::all_of(op.getInteriorPadding(),
                      [](const int64_t &i) { return i == 0; })) {
-      auto padTensorOp = rewriter.create<tensor::PadOp>(
-          loc, resultType, adaptor.getOperand(),
+      auto padTensorOp = tensor::PadOp::create(
+          rewriter, loc, resultType, adaptor.getOperand(),
           llvm::map_to_vector(op.getEdgePaddingLow(), i64ToFoldResult),
           llvm::map_to_vector(op.getEdgePaddingHigh(), i64ToFoldResult),
           paddingVal);
@@ -2424,7 +2431,7 @@
     auto emptyTensor =
         getEmptyTensorFor(rewriter, loc, resultType, op, adaptor.getOperands());
     auto fill =
-        rewriter.create<linalg::FillOp>(loc, paddingVal, emptyTensor).result();
+        linalg::FillOp::create(rewriter, loc, paddingVal, emptyTensor).result();
 
     // Get sizes of the original operand.
     auto operandType = llvm::cast<ShapedType>(adaptor.getOperand().getType());
@@ -2433,7 +2440,7 @@
         [&](int64_t dim) -> OpFoldResult {
           if (!operandType.isDynamicDim(dim))
             return rewriter.getIndexAttr(operandType.getDimSize(dim));
-          return rewriter.create<tensor::DimOp>(loc, adaptor.getOperand(), dim)
+          return tensor::DimOp::create(rewriter, loc, adaptor.getOperand(), dim)
               .getResult();
         });
     // Map interior padding to strides.
@@ -2481,15 +2488,15 @@
       if (!resultType.isDynamicDim(i)) continue;
       if (i < axis) {
         dynSizes.push_back(
-            rewriter.create<tensor::DimOp>(loc, adaptor.getOperand(), i));
+            tensor::DimOp::create(rewriter, loc, adaptor.getOperand(), i));
       } else if (i < (axis + numIndices - batch)) {
         int idx = i - axis + batch;
         dynSizes.push_back(
-            rewriter.create<tensor::DimOp>(loc, adaptor.getIndex(), idx));
+            tensor::DimOp::create(rewriter, loc, adaptor.getIndex(), idx));
       } else {
         int idx = i - (axis + numIndices - batch) + axis + 1;
         dynSizes.push_back(
-            rewriter.create<tensor::DimOp>(loc, adaptor.getOperand(), idx));
+            tensor::DimOp::create(rewriter, loc, adaptor.getOperand(), idx));
       }
     }
 
@@ -2503,7 +2510,7 @@
       sliceShape.push_back(resultShape[i]);
       if (!resultType.isDynamicDim(i)) continue;
       dynSliceSizes.push_back(
-          rewriter.create<tensor::DimOp>(loc, adaptor.getOperand(), i));
+          tensor::DimOp::create(rewriter, loc, adaptor.getOperand(), i));
     }
     for (int i = axis + numIndices - batch; i < rank; ++i) {
       sliceExprs.push_back(rewriter.getAffineDimExpr(i));
@@ -2511,7 +2518,7 @@
       if (!resultType.isDynamicDim(i)) continue;
       int idx = i - (axis + numIndices - batch) + axis + 1;
       dynSliceSizes.push_back(
-          rewriter.create<tensor::DimOp>(loc, adaptor.getOperand(), idx));
+          tensor::DimOp::create(rewriter, loc, adaptor.getOperand(), idx));
     }
 
     // Setup AffineMap for operand tensor.
@@ -2530,13 +2537,14 @@
         rank, /*symbolCount=*/0, sliceExprs, rewriter.getContext()));
     indexingMaps.emplace_back(rewriter.getMultiDimIdentityMap(rank));
 
-    Value sliceOp = rewriter.create<tensor::EmptyOp>(
-        loc, sliceShape, resultType.getElementType(), dynSliceSizes);
-
-    Value emptyOp = rewriter.create<tensor::EmptyOp>(
-        loc, resultType.getShape(), resultType.getElementType(), dynSizes);
-    auto linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, /*resultTensors=*/ArrayRef<Type>{resultType},
+    Value sliceOp = tensor::EmptyOp::create(
+        rewriter, loc, sliceShape, resultType.getElementType(), dynSliceSizes);
+
+    Value emptyOp =
+        tensor::EmptyOp::create(rewriter, loc, resultType.getShape(),
+                                resultType.getElementType(), dynSizes);
+    auto linalgOp = linalg::GenericOp::create(
+        rewriter, loc, /*resultTensors=*/ArrayRef<Type>{resultType},
         /*inputs=*/ValueRange{adaptor.getIndex(), sliceOp},
         /*outputs=*/emptyOp, indexingMaps, getNParallelLoopsAttrs(rank),
         /*bodyBuild=*/nullptr, linalg::getPrunedAttributeList(op));
@@ -2556,20 +2564,20 @@
     OpBuilder::InsertionGuard guard(rewriter);
     rewriter.setInsertionPointToEnd(block);
 
-    Value castedValue = rewriter.create<arith::IndexCastOp>(
-        loc, rewriter.getIndexType(), block->getArgument(0));
+    Value castedValue = arith::IndexCastOp::create(
+        rewriter, loc, rewriter.getIndexType(), block->getArgument(0));
 
     SmallVector<Value> indices;
     for (int i = 0; i < axis; ++i) {
-      indices.push_back(rewriter.create<linalg::IndexOp>(loc, i));
+      indices.push_back(linalg::IndexOp::create(rewriter, loc, i));
     }
     indices.push_back(castedValue);
     for (int i = axis + numIndices - batch; i < rank; ++i) {
-      indices.push_back(rewriter.create<linalg::IndexOp>(loc, i));
+      indices.push_back(linalg::IndexOp::create(rewriter, loc, i));
     }
     Value res =
-        rewriter.create<tensor::ExtractOp>(loc, adaptor.getOperand(), indices);
-    rewriter.create<linalg::YieldOp>(loc, res);
+        tensor::ExtractOp::create(rewriter, loc, adaptor.getOperand(), indices);
+    linalg::YieldOp::create(rewriter, loc, res);
 
     rewriter.replaceOp(op, linalgOp.getResults());
     return success();
@@ -2598,11 +2606,10 @@
                                        return rewriter.getIndexAttr(dim);
                                      });
     Value dimensionSize =
-        rewriter.create<tensor::ExtractOp>(loc, setDimensionSizeOp.getSize());
+        tensor::ExtractOp::create(rewriter, loc, setDimensionSizeOp.getSize());
     sizes[setDimensionSizeOp.getDimension()] =
-        rewriter
-            .create<arith::IndexCastOp>(loc, rewriter.getIndexType(),
-                                        dimensionSize)
+        arith::IndexCastOp::create(rewriter, loc, rewriter.getIndexType(),
+                                   dimensionSize)
             .getResult();
 
     rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgConvolution.cpp
@@ -97,17 +97,17 @@
   if (auto complexType = dyn_cast<ComplexType>(inputType.getElementType())) {
     auto zeroElement = rewriter.getZeroAttr(complexType.getElementType());
     auto zeroAttr = rewriter.getArrayAttr({zeroElement, zeroElement});
-    zero = rewriter.create<complex::ConstantOp>(loc, complexType, zeroAttr);
-    zero = rewriter.create<tensor::FromElementsOp>(
-        loc, RankedTensorType::get({}, complexType), zero);
+    zero = complex::ConstantOp::create(rewriter, loc, complexType, zeroAttr);
+    zero = tensor::FromElementsOp::create(
+        rewriter, loc, RankedTensorType::get({}, complexType), zero);
   } else {
-    zero = rewriter.create<arith::ConstantOp>(
-        loc, rewriter.getZeroAttr(
-                 RankedTensorType::get({}, inputType.getElementType())));
-  }
-
-  return rewriter.create<mlir::stablehlo::PadOp>(loc, input, zero, padLow,
-                                                 padHigh, padInterior);
+    zero = arith::ConstantOp::create(rewriter, loc,
+                                     rewriter.getZeroAttr(RankedTensorType::get(
+                                         {}, inputType.getElementType())));
+  }
+
+  return mlir::stablehlo::PadOp::create(rewriter, loc, input, zero, padLow,
+                                        padHigh, padInterior);
 }
 
 /// If the ConvolutionOp has a window reversal, applies it to the filter.
@@ -126,8 +126,8 @@
     }
   }
 
-  return b.create<mlir::stablehlo::ReverseOp>(
-      loc, filter, b.getDenseI64ArrayAttr(reversedDims));
+  return mlir::stablehlo::ReverseOp::create(
+      b, loc, filter, b.getDenseI64ArrayAttr(reversedDims));
 }
 
 /// Returns true if the given `dimensionNumbers` from a stablehlo.convolution op
@@ -231,7 +231,7 @@
     // The output shape is N spatial_dims F.
     SmallVector<Value, 8> dynSizes;
     if (resultType.isDynamicDim(0)) {
-      dynSizes.push_back(rewriter.create<tensor::DimOp>(loc, input, 0));
+      dynSizes.push_back(tensor::DimOp::create(rewriter, loc, input, 0));
     }
     for (int64_t i = 1, e = rank - 1; i < e; ++i) {
       if (resultType.isDynamicDim(i)) {
@@ -240,10 +240,12 @@
       }
     }
     if (resultType.isDynamicDim(rank - 1)) {
-      dynSizes.push_back(rewriter.create<tensor::DimOp>(loc, filter, rank - 1));
-    }
-    Value emptyTensor = rewriter.create<tensor::EmptyOp>(
-        loc, resultType.getShape(), resultType.getElementType(), dynSizes);
+      dynSizes.push_back(
+          tensor::DimOp::create(rewriter, loc, filter, rank - 1));
+    }
+    Value emptyTensor =
+        tensor::EmptyOp::create(rewriter, loc, resultType.getShape(),
+                                resultType.getElementType(), dynSizes);
     Value zeroTensor = fillTensorWithZeros(rewriter, loc, emptyTensor);
     linalg::LinalgOp res;
     Attribute strides;
@@ -260,27 +262,30 @@
 
     switch (rank) {
       case 2: {
-        res = rewriter.create<linalg::MatmulOp>(
-            loc, resultType, ValueRange{input, filter}, ValueRange{zeroTensor},
+        res = linalg::MatmulOp::create(
+            rewriter, loc, resultType, ValueRange{input, filter},
+            ValueRange{zeroTensor}, linalg::getPrunedAttributeList(op));
+        break;
+      }
+      case 3: {
+        res = linalg::Conv1DNwcWcfOp::create(
+            rewriter, loc, resultType, ValueRange{input, filter},
+            ValueRange{zeroTensor}, strides, dilations,
             linalg::getPrunedAttributeList(op));
         break;
       }
-      case 3: {
-        res = rewriter.create<linalg::Conv1DNwcWcfOp>(
-            loc, resultType, ValueRange{input, filter}, ValueRange{zeroTensor},
-            strides, dilations, linalg::getPrunedAttributeList(op));
+      case 4: {
+        res = linalg::Conv2DNhwcHwcfOp::create(
+            rewriter, loc, resultType, ValueRange{input, filter},
+            ValueRange{zeroTensor}, strides, dilations,
+            linalg::getPrunedAttributeList(op));
         break;
       }
-      case 4: {
-        res = rewriter.create<linalg::Conv2DNhwcHwcfOp>(
-            loc, resultType, ValueRange{input, filter}, ValueRange{zeroTensor},
-            strides, dilations, linalg::getPrunedAttributeList(op));
-        break;
-      }
       case 5: {
-        res = rewriter.create<linalg::Conv3DNdhwcDhwcfOp>(
-            loc, resultType, ValueRange{input, filter}, ValueRange{zeroTensor},
-            strides, dilations, linalg::getPrunedAttributeList(op));
+        res = linalg::Conv3DNdhwcDhwcfOp::create(
+            rewriter, loc, resultType, ValueRange{input, filter},
+            ValueRange{zeroTensor}, strides, dilations,
+            linalg::getPrunedAttributeList(op));
         break;
       }
       default: {
@@ -438,8 +443,8 @@
         reshapeShapeVector(prevDimsRef, newShape, inputFeatureDimension,
                            featureGroupCount);
         updateDimMappingFromOffset(lhsIndexMapping, inputFeatureDimension);
-        modifiedLhs = rewriter.create<mlir::stablehlo::ReshapeOp>(
-            loc,
+        modifiedLhs = mlir::stablehlo::ReshapeOp::create(
+            rewriter, loc,
             RankedTensorType::get(newShape, paddedLhsType.getElementType()),
             modifiedLhs);
       }
@@ -454,8 +459,8 @@
                            featureGroupCount);
         updateDimMappingFromOffset(rhsIndexMapping,
                                    kernelOutputFeatureDimension);
-        modifiedRhs = rewriter.create<mlir::stablehlo::ReshapeOp>(
-            loc,
+        modifiedRhs = mlir::stablehlo::ReshapeOp::create(
+            rewriter, loc,
             RankedTensorType::get(newShape, paddedRhsType.getElementType()),
             modifiedRhs);
       }
@@ -481,8 +486,8 @@
         reshapeShapeVector(prevDimsRef, newShape, inputBatchDimension,
                            batchGroupCount);
         updateDimMappingFromOffset(lhsIndexMapping, inputBatchDimension);
-        modifiedLhs = rewriter.create<mlir::stablehlo::ReshapeOp>(
-            op.getLoc(),
+        modifiedLhs = mlir::stablehlo::ReshapeOp::create(
+            rewriter, op.getLoc(),
             RankedTensorType::get(newShape, paddedLhsType.getElementType()),
             modifiedLhs);
       }
@@ -497,8 +502,8 @@
                            batchGroupCount);
         updateDimMappingFromOffset(rhsIndexMapping,
                                    kernelOutputFeatureDimension);
-        modifiedRhs = rewriter.create<mlir::stablehlo::ReshapeOp>(
-            op.getLoc(),
+        modifiedRhs = mlir::stablehlo::ReshapeOp::create(
+            rewriter, op.getLoc(),
             RankedTensorType::get(newShape, paddedRhsType.getElementType()),
             modifiedRhs);
       }
@@ -562,28 +567,26 @@
     auto inferredMaps =
         AffineMap::inferFromExprList({srcExprs, windowExprs, dstExprs}, ctx);
 
-    Value emptyTensor = rewriter.create<tensor::EmptyOp>(
-        loc, reshapedResultShape, resultType.getElementType());
+    Value emptyTensor = tensor::EmptyOp::create(
+        rewriter, loc, reshapedResultShape, resultType.getElementType());
     Value zeroTensor = fillTensorWithZeros(rewriter, loc, emptyTensor);
 
     Value convolved =
-        rewriter
-            .create<linalg::GenericOp>(
-                loc,
-                /*resultTensors=*/
-                llvm::ArrayRef<Type>(zeroTensor.getType()),
-                /*inputs=*/
-                llvm::ArrayRef<Value>({modifiedLhs, modifiedRhs}),
-                /*outputs=*/llvm::ArrayRef<Value>(zeroTensor), inferredMaps,
-                iterationLoops,
-                /*bodyBuild=*/
-                [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange) {
-                  ImplicitLocOpBuilder builder(nestedLoc, nestedBuilder);
-                  linalg::Conv2DOp::regionBuilder(builder,
-                                                  *builder.getInsertionBlock(),
-                                                  {}, /*emitError=*/{});
-                },
-                linalg::getPrunedAttributeList(op))
+        linalg::GenericOp::create(
+            rewriter, loc,
+            /*resultTensors=*/
+            llvm::ArrayRef<Type>(zeroTensor.getType()),
+            /*inputs=*/
+            llvm::ArrayRef<Value>({modifiedLhs, modifiedRhs}),
+            /*outputs=*/llvm::ArrayRef<Value>(zeroTensor), inferredMaps,
+            iterationLoops,
+            /*bodyBuild=*/
+            [&](OpBuilder &nestedBuilder, Location nestedLoc, ValueRange) {
+              ImplicitLocOpBuilder builder(nestedLoc, nestedBuilder);
+              linalg::Conv2DOp::regionBuilder(
+                  builder, *builder.getInsertionBlock(), {}, /*emitError=*/{});
+            },
+            linalg::getPrunedAttributeList(op))
             .getResult(0);
     rewriter.replaceOpWithNewOp<mlir::stablehlo::ReshapeOp>(op, resultType,
                                                             convolved);
@@ -709,8 +712,8 @@
             reshapedFilterDims,
             cast<ShapedType>(op.getRhs().getType()).getElementType());
 
-        reshapedFilter = rewriter.create<mlir::stablehlo::ReshapeOp>(
-            loc, reshapedFilterType, filter);
+        reshapedFilter = mlir::stablehlo::ReshapeOp::create(
+            rewriter, loc, reshapedFilterType, filter);
       }
 
       ArrayRef<int64_t> outputDims = resultType.getShape();
@@ -720,8 +723,8 @@
       reshapedOutputDims.push_back(channelMultiplier);
       reshapedOutputDims[reshapedOutputDims.size() - 2] /= channelMultiplier;
 
-      Value emptyTensor = rewriter.create<tensor::EmptyOp>(
-          loc, reshapedOutputDims, resultType.getElementType());
+      Value emptyTensor = tensor::EmptyOp::create(
+          rewriter, loc, reshapedOutputDims, resultType.getElementType());
       Value zeroTensor = fillTensorWithZeros(rewriter, loc, emptyTensor);
 
       auto reshapedOutputType = RankedTensorType::get(
@@ -729,32 +732,29 @@
       Value conv;
       switch (spatialRank) {
         case 1: {
-          conv = rewriter
-                     .create<linalg::DepthwiseConv1DNwcWcmOp>(
-                         loc, reshapedOutputType,
-                         ValueRange{input, reshapedFilter},
-                         ValueRange{zeroTensor}, windowStrides, rhsDilation,
-                         linalg::getPrunedAttributeList(op))
+          conv = linalg::DepthwiseConv1DNwcWcmOp::create(
+                     rewriter, loc, reshapedOutputType,
+                     ValueRange{input, reshapedFilter}, ValueRange{zeroTensor},
+                     windowStrides, rhsDilation,
+                     linalg::getPrunedAttributeList(op))
                      .getResult(0);
           break;
         }
         case 2: {
-          conv = rewriter
-                     .create<linalg::DepthwiseConv2DNhwcHwcmOp>(
-                         loc, reshapedOutputType,
-                         ValueRange{input, reshapedFilter},
-                         ValueRange{zeroTensor}, windowStrides, rhsDilation,
-                         linalg::getPrunedAttributeList(op))
+          conv = linalg::DepthwiseConv2DNhwcHwcmOp::create(
+                     rewriter, loc, reshapedOutputType,
+                     ValueRange{input, reshapedFilter}, ValueRange{zeroTensor},
+                     windowStrides, rhsDilation,
+                     linalg::getPrunedAttributeList(op))
                      .getResult(0);
           break;
         }
         case 3: {
-          conv = rewriter
-                     .create<linalg::DepthwiseConv3DNdhwcDhwcmOp>(
-                         loc, reshapedOutputType,
-                         ValueRange{input, reshapedFilter},
-                         ValueRange{zeroTensor}, windowStrides, rhsDilation,
-                         linalg::getPrunedAttributeList(op))
+          conv = linalg::DepthwiseConv3DNdhwcDhwcmOp::create(
+                     rewriter, loc, reshapedOutputType,
+                     ValueRange{input, reshapedFilter}, ValueRange{zeroTensor},
+                     windowStrides, rhsDilation,
+                     linalg::getPrunedAttributeList(op))
                      .getResult(0);
           break;
         }
@@ -771,8 +771,8 @@
           getReassociationIndicesToCollapseLastTwoDims(conv));
     } else {
       // For cases where channel multiplier == 1
-      Value emptyTensor = rewriter.create<tensor::EmptyOp>(
-          loc, resultType.getShape(), resultType.getElementType());
+      Value emptyTensor = tensor::EmptyOp::create(
+          rewriter, loc, resultType.getShape(), resultType.getElementType());
       Value zeroTensor = fillTensorWithZeros(rewriter, loc, emptyTensor);
 
       // Create a Linalg reshape op that converts the filter from 4 dimensions
@@ -787,8 +787,8 @@
       RankedTensorType filterShape =
           RankedTensorType::get(filterDims, op.getType().getElementType());
 
-      Value reshapedFilter = rewriter.create<tensor::CollapseShapeOp>(
-          loc, filterShape, filter,
+      Value reshapedFilter = tensor::CollapseShapeOp::create(
+          rewriter, loc, filterShape, filter,
           getReassociationIndicesToCollapseLastTwoDims(filter));
 
       switch (spatialRank) {
diff --ruN a/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgDotProduct.cpp b/stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgDotProduct.cpp
--- stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgDotProduct.cpp
+++ stablehlo/stablehlo/conversions/linalg/transforms/StablehloToLinalgDotProduct.cpp
@@ -89,9 +89,9 @@
   }
 
   if (lhsIsMatrix && lhsType.isDynamicDim(0))
-    dynShape.push_back(b.create<tensor::DimOp>(loc, lhs, 0));
+    dynShape.push_back(tensor::DimOp::create(b, loc, lhs, 0));
   if (rhsIsMatrix && rhsType.isDynamicDim(1))
-    dynShape.push_back(b.create<tensor::DimOp>(loc, rhs, 1));
+    dynShape.push_back(tensor::DimOp::create(b, loc, rhs, 1));
   return dynShape;
 }
 
@@ -182,8 +182,8 @@
     Value emptyTensor =
         getEmptyTensorFor(rewriter, loc, outputType, op, adaptor.getOperands());
     Value zeroTensor = fillTensorWithZeros(rewriter, loc, emptyTensor);
-    Operation *linalgOp = rewriter.create<linalg::BatchMatmulOp>(
-        loc, /*resultTensorTypes=*/TypeRange{outputType},
+    Operation *linalgOp = linalg::BatchMatmulOp::create(
+        rewriter, loc, /*resultTensorTypes=*/TypeRange{outputType},
         /*inputs=*/ValueRange{adaptor.getLhs(), adaptor.getRhs()},
         /*outputBuffers=*/ValueRange{zeroTensor},
         linalg::getPrunedAttributeList(op));
@@ -290,8 +290,8 @@
                                             op.getContext()));
     }
 
-    Operation *linalgOp = rewriter.create<linalg::GenericOp>(
-        loc, /*resultTensorTypes=*/TypeRange{outputType},
+    Operation *linalgOp = linalg::GenericOp::create(
+        rewriter, loc, /*resultTensorTypes=*/TypeRange{outputType},
         /*inputs=*/ValueRange{adaptor.getLhs(), adaptor.getRhs()},
         /*outputBuffers=*/ValueRange{zeroTensor}, indexingMaps,
         getParallelAndReductionIterators(
diff --ruN a/stablehlo/stablehlo/dialect/Base.cpp b/stablehlo/stablehlo/dialect/Base.cpp
--- stablehlo/stablehlo/dialect/Base.cpp
+++ stablehlo/stablehlo/dialect/Base.cpp
@@ -798,5 +798,35 @@
   return numBoundedDims == 1 && numDynamicDims == 1;
 }
 
+//===----------------------------------------------------------------------===//
+// Utils for type traversal.
+//===----------------------------------------------------------------------===//
+
+namespace {
+LogicalResult mapOverLeafTypesImpl(
+    Type type, function_ref<LogicalResult(Type type, ArrayRef<int64_t>)> fn,
+    std::vector<int64_t>& indices) {
+  if (!isa<TupleType>(type)) {
+    return fn(type, indices);
+  }
+
+  auto tupleType = cast<TupleType>(type);
+  for (size_t i = 0; i < tupleType.size(); ++i) {
+    indices.push_back(i);
+    if (failed(mapOverLeafTypesImpl(tupleType.getType(i), fn, indices)))
+      return failure();
+    indices.pop_back();
+  }
+
+  return success();
+}
+}  // namespace
+
+LogicalResult mapOverLeafTypes(
+    Type type, function_ref<LogicalResult(Type, ArrayRef<int64_t>)> fn) {
+  std::vector<int64_t> indices;
+  return mapOverLeafTypesImpl(type, fn, indices);
+}
+
 }  // namespace hlo
 }  // namespace mlir
diff --ruN a/stablehlo/stablehlo/dialect/Base.h b/stablehlo/stablehlo/dialect/Base.h
--- stablehlo/stablehlo/dialect/Base.h
+++ stablehlo/stablehlo/dialect/Base.h
@@ -314,6 +314,11 @@
 mlir::Speculation::Speculatability getShapedSpeculatability(Operation *op,
                                                             int64_t shapeCount);
 
+// Applies `fn` to `type` if it is not a `tuple` type. Otherwise, applies `fn`
+// to each leaf type in the `tuple` type tree or until a `fn` returns failure.
+LogicalResult mapOverLeafTypes(
+    Type type, function_ref<LogicalResult(Type, ArrayRef<int64_t>)> fn);
+
 namespace OpTrait {
 
 template <typename ConcreteType>
diff --ruN a/stablehlo/stablehlo/dialect/Base.td b/stablehlo/stablehlo/dialect/Base.td
--- stablehlo/stablehlo/dialect/Base.td
+++ stablehlo/stablehlo/dialect/Base.td
@@ -193,15 +193,21 @@
 
 def HLO_ComplexTensor : RankedTensorOf<[HLO_Complex]>;
 
-def HLO_Tuple : NestedTupleOf<[HLO_Tensor, HLO_PerAxisQuantizedIntTensor, HLO_Token]>;
+def HLO_Buffer : MemRefOf<[HLO_Float, HLO_Pred, HLO_Int, HLO_Complex, HLO_QuantizedInt]>;
+
+def HLO_Tuple : NestedTupleOf<[HLO_Tensor, HLO_Buffer, HLO_PerAxisQuantizedIntTensor, HLO_Token]>;
 
 def HLO_TensorOrToken : AnyTypeOf<[HLO_Tensor, HLO_Token]>;
 
 def HLO_TensorOrPerAxisQuantizedTensorOrToken : AnyTypeOf<[HLO_Tensor, HLO_PerAxisQuantizedIntTensor, HLO_Token]>;
 
+def HLO_TensorOrPerAxisQuantizedTensorOrTokenOrBuffer : AnyTypeOf<[HLO_TensorOrPerAxisQuantizedTensorOrToken, HLO_Buffer]>;
+
 def HLO_TensorOrTokenOrTuple : AnyTypeOf<[HLO_Tensor, HLO_Token, HLO_Tuple]>;
 
 def HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTuple : AnyTypeOf<[HLO_Tensor, HLO_PerAxisQuantizedIntTensor, HLO_Token, HLO_Tuple]>;
+
+def HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTupleOrBuffer : AnyTypeOf<[HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTuple, HLO_Buffer]>;
 
 def HLO_DimensionValue : AnyTypeOf<[Index, HLO_Int]>;
 
@@ -227,9 +233,9 @@
 
 def HLO_AnyPredOrIntTensor : TensorOf<[HLO_Pred, HLO_Int]>;
 
-def HLO_AnyTuple : NestedTupleOf<[HLO_AnyTensor, HLO_Token]>;
-
-def HLO_CustomCallValue : AnyTypeOf<[HLO_AnyTensor, HLO_Token, HLO_AnyTuple]>;
+def HLO_CustomCallTuple : NestedTupleOf<[HLO_AnyTensor, HLO_Buffer, HLO_Token]>;
+
+def HLO_CustomCallValue : AnyTypeOf<[HLO_AnyTensor, HLO_Buffer, HLO_Token, HLO_CustomCallTuple]>;
 
 //===----------------------------------------------------------------------===//
 // HLO combined type definitions.
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.cpp b/stablehlo/stablehlo/dialect/StablehloOps.cpp
--- stablehlo/stablehlo/dialect/StablehloOps.cpp
+++ stablehlo/stablehlo/dialect/StablehloOps.cpp
@@ -535,6 +535,133 @@
     }
   }
 
+  // We have already verified that the output_operand_aliases have consistent
+  // types and valid indices. Here we verify buffer releated special custom_call
+  // targets, and also verify that buffer operands used non-special custom_call
+  // ops meet this requirements:
+  //   A result with a buffer type should be mentioned in one pair of
+  //   output-operand-aliasing and an operand with a buffer type can only appear
+  //   at most once in output-operand-aliasing.
+  if (getCallTargetName() == kCreateBufferCustomCallTarget) {
+    if (!getInputs().empty()) {
+      return emitOpError()
+             << "CreateBuffer custom_call shouldn't have any inputs";
+    }
+    if (getNumResults() != 1) {
+      return emitOpError() << "CreateBuffer custom_call should have one result";
+    }
+    if (!isa<MemRefType>(getResult(0).getType())) {
+      return emitOpError()
+             << "CreateBuffer custom_call should have a memref result";
+    }
+    return success();
+  }
+
+  if (getCallTargetName() == kPinCustomCallTarget) {
+    if (getInputs().size() != 1) {
+      return emitOpError() << "Pin custom_call should have one input";
+    }
+    Type inputType = getInputs()[0].getType();
+    if (!isa<TensorType>(inputType)) {
+      return emitOpError() << "Pin custom_call should have a tensor input";
+    }
+    if (getNumResults() != 1) {
+      return emitOpError() << "Pin custom_call should have one output";
+    }
+    Type outputType = getResult(0).getType();
+    if (!isa<MemRefType>(outputType)) {
+      return emitOpError() << "Pin custom_call should have a memref output";
+    }
+
+    if (failed(verifyCompatibleShape(inputType, outputType)) ||
+        getElementTypeOrSelf(inputType) != getElementTypeOrSelf(outputType)) {
+      return emitOpError()
+             << "Pin custom_call should have compatible input and output types";
+    }
+
+    return success();
+  }
+
+  if (getCallTargetName() == kUnpinCustomCallTarget) {
+    if (getInputs().size() != 1) {
+      return emitOpError() << "Unpin custom_call should have one input";
+    }
+    Type inputType = getInputs()[0].getType();
+    if (!isa<MemRefType>(inputType)) {
+      return emitOpError() << "Unpin custom_call should have a memref input";
+    }
+    if (getNumResults() != 1) {
+      return emitOpError() << "Unpin custom_call should have one output";
+    }
+    Type outputType = getResult(0).getType();
+    if (!isa<TensorType>(outputType)) {
+      return emitOpError() << "Unpin custom_call should have a tensor output";
+    }
+
+    if (failed(verifyCompatibleShape(inputType, outputType)) ||
+        getElementTypeOrSelf(inputType) != getElementTypeOrSelf(outputType)) {
+      return emitOpError() << "Unpin custom_call should have compatible input "
+                              "and output types";
+    }
+
+    return success();
+  }
+
+  auto getOperandPartType = [&](int64_t operandIndex,
+                                ::llvm::ArrayRef<int64_t> operandTupleIndices) {
+    Type operandPart = getOperand(operandIndex).getType();
+    for (auto i : operandTupleIndices) {
+      operandPart = cast<TupleType>(operandPart).getType(i);
+    }
+    return operandPart;
+  };
+  auto printIndices = [](ArrayRef<int64_t> indices) {
+    std::vector<std::string> stringIndices;
+    llvm::transform(indices, std::back_inserter(stringIndices),
+                    [](int v) { return llvm::Twine(v).str(); });
+    return "[" + llvm::join(stringIndices, ", ") + "]";
+  };
+
+  std::set<std::pair<int64_t, std::vector<int64_t>>> operandParts;
+  std::set<std::vector<int64_t>> outputParts;
+  for (auto attr : aliasArrayAttr) {
+    auto alias = cast<OutputOperandAliasAttr>(attr);
+    auto outputTupleIndices = alias.getOutputTupleIndices();
+    auto operandIndex = alias.getOperandIndex();
+    auto operandTupleIndices = alias.getOperandTupleIndices();
+    Type operandPartType =
+        getOperandPartType(operandIndex, operandTupleIndices);
+    if (!isa<MemRefType>(operandPartType)) continue;
+
+    if (!outputParts.insert(outputTupleIndices).second) {
+      emitOpError() << "buffer output " + printIndices(outputTupleIndices) +
+                           " is used in output_operand_alias more than once";
+    }
+    if (!operandParts.insert(std::make_pair(operandIndex, operandTupleIndices))
+             .second) {
+      emitOpError() << "buffer operand " + std::to_string(operandIndex) +
+                           printIndices(operandTupleIndices) +
+                           " is used in output_operand_alias more than once";
+    }
+  }
+
+  Type resultType = getNumResults() == 1
+                        ? getResult(0).getType()
+                        : TupleType::get(getContext(), getResultTypes());
+  if (failed(hlo::mapOverLeafTypes(
+          resultType,
+          [&](Type type, ArrayRef<int64_t> indices) -> LogicalResult {
+            if (!isa<MemRefType>(type)) return success();
+            if (!outputParts.contains(indices)) {
+              return emitOpError()
+                     << "buffer output " + printIndices(indices) +
+                            " is not used in output_operand_alias";
+            }
+            return success();
+          }))) {
+    return failure();
+  }
+
   return success();
 }
 
@@ -3852,14 +3979,19 @@
 //
 // Note that this right now only does comparision on the first pair of block
 // arguments.
-static void buildSortComparisonBody(llvm::ArrayRef<Type> elementTypes,
-                                    ComparisonDirection direction,
-                                    std::optional<StringRef> compareType,
-                                    Region* body, OpBuilder* builder) {
+void buildSortComparisonBody(llvm::ArrayRef<Type> elementTypes,
+                             ComparisonDirection direction,
+                             std::optional<StringRef> compareType, Region* body,
+                             OpBuilder* builder) {
   OpBuilder::InsertionGuard insertionPointGuard(*builder);
 
   Location loc = body->getLoc();
-  Block* block = builder->createBlock(body);
+  Block* block;
+  if (body->empty()) {
+    block = builder->createBlock(body);
+  } else {
+    block = &body->front();
+  }
   // Add two arguments for each element type.
   for (Type elementType : elementTypes) {
     ShapedType shapedType = RankedTensorType::get({}, elementType);
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.h b/stablehlo/stablehlo/dialect/StablehloOps.h
--- stablehlo/stablehlo/dialect/StablehloOps.h
+++ stablehlo/stablehlo/dialect/StablehloOps.h
@@ -172,6 +172,16 @@
 // a DotOp, given the LHS of such an operation.
 DotDimensionNumbersAttr getDefaultDotDimensionNumbers(mlir::Value lhs);
 
+// Builds the region `body` for stablehlo.sort's comparator: for each type in
+// `element_types`, create two block arguments, one for lhs and one for rhs, and
+// generates stablehlo.compare op to compare them with the given `direction`.
+// Note that this right now only does comparision on the first pair of block
+// arguments.
+void buildSortComparisonBody(llvm::ArrayRef<Type> elementTypes,
+                             ComparisonDirection direction,
+                             std::optional<StringRef> compareType, Region *body,
+                             OpBuilder *builder);
+
 SortOp createSortOp(PatternRewriter *rewriter, const Location &loc,
                     const llvm::ArrayRef<Value> &operands,
                     const llvm::ArrayRef<Type> &elementTypes, int64_t dimension,
@@ -200,6 +210,9 @@
   static ArrayAttr get(MLIRContext *context, ArrayRef<Precision> precisions);
 };
 
+constexpr StringRef kCreateBufferCustomCallTarget = "CreateBuffer";
+constexpr StringRef kPinCustomCallTarget = "Pin";
+constexpr StringRef kUnpinCustomCallTarget = "Unpin";
 }  // end namespace stablehlo
 }  // end namespace mlir
 
diff --ruN a/stablehlo/stablehlo/dialect/StablehloOps.td b/stablehlo/stablehlo/dialect/StablehloOps.td
--- stablehlo/stablehlo/dialect/StablehloOps.td
+++ stablehlo/stablehlo/dialect/StablehloOps.td
@@ -1245,11 +1245,6 @@
   );
 
   let results = (outs HLO_Token);
-  let builders = [
-    OpBuilder<(ins
-      "::mlir::Type":$result_type, "::mlir::Value":$operand,
-      "::mlir::DenseIntElementsAttr":$source_target_pairs,
-      "::mlir::stablehlo::ChannelHandleAttr":$channel_handle)>];
 }
 
 def StableHLO_RecvOp : StableHLO_Op<"recv", [
@@ -1279,11 +1274,6 @@
     DefaultValuedOptionalAttr<BoolAttr, "false">:$is_host_transfer, /*recv_i4*/
     OptionalAttr<I64ElementsAttr>:$source_target_pairs /*recv_i5*/
   );
-  let builders = [
-    OpBuilder<(ins
-      "::mlir::Type":$result_type, "::mlir::Value":$operand,
-      "::mlir::DenseIntElementsAttr":$source_target_pairs,
-      "::mlir::stablehlo::ChannelHandleAttr":$channel_handle)>];
 
   let results = (outs Variadic<HLO_StaticShapeTensorOrPerAxisQuantizedTensorOrToken>);
   let hasVerifier = 1;
@@ -1463,14 +1453,14 @@
     }
     ```
   }];
-  let arguments = (ins Variadic<HLO_TensorOrPerAxisQuantizedTensorOrToken>:$operand /*while_i1*/);
+  let arguments = (ins Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrBuffer>:$operand /*while_i1*/);
 
   let regions = (region
     SizedRegion<1>:$cond /*while_i2*/,
     SizedRegion<1>:$body /*while_i3*/
   );
 
-  let results = (outs Variadic<HLO_TensorOrPerAxisQuantizedTensorOrToken>);
+  let results = (outs Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrBuffer>);
 
   let hasVerifier = 1;
 
@@ -1736,7 +1726,7 @@
     ConfinedAttr<I32Attr, [IntNonNegative]>:$index /*get_tuple_element_c1, get_tuple_element_i2*/
   );
 
-  let results = (outs HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTuple);
+  let results = (outs HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTupleOrBuffer);
 
   let assemblyFormat = [{
     $operand `[` $index `]` attr-dict `:` functional-type(operands, results)
@@ -1758,7 +1748,7 @@
     ```
    }];
 
-  let arguments = (ins Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTuple>:$val /*tuple_i1*/);
+  let arguments = (ins Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrTupleOrBuffer>:$val /*tuple_i1*/);
   let results = (outs HLO_Tuple:$result);
 
   let assemblyFormat = [{
@@ -3356,7 +3346,7 @@
   }];
 
   let arguments = (ins
-    Variadic<HLO_TensorOrPerAxisQuantizedTensorOrToken>:$results
+    Variadic<HLO_TensorOrPerAxisQuantizedTensorOrTokenOrBuffer>:$results
   );
 
   let assemblyFormat = "$results attr-dict (`:` type($results)^)?";
diff --ruN a/stablehlo/stablehlo/dialect/Version.cpp b/stablehlo/stablehlo/dialect/Version.cpp
--- stablehlo/stablehlo/dialect/Version.cpp
+++ stablehlo/stablehlo/dialect/Version.cpp
@@ -83,7 +83,7 @@
     case CompatibilityRequirement::NONE:
       return Version::getCurrentVersion();
     case CompatibilityRequirement::WEEK_4:
-      return Version(1, 10, 10);  // WEEK_4 ANCHOR: DO NOT MODIFY
+      return Version(1, 11, 0);  // WEEK_4 ANCHOR: DO NOT MODIFY
     case CompatibilityRequirement::WEEK_12:
       return Version(1, 10, 3);  // WEEK_12 ANCHOR: DO NOT MODIFY
     case CompatibilityRequirement::MAX:
diff --ruN a/stablehlo/stablehlo/dialect/Version.h b/stablehlo/stablehlo/dialect/Version.h
--- stablehlo/stablehlo/dialect/Version.h
+++ stablehlo/stablehlo/dialect/Version.h
@@ -38,7 +38,7 @@
   static FailureOr<Version> fromString(llvm::StringRef versionRef);
 
   /// Return a Version representing the current VHLO dialect version.
-  static Version getCurrentVersion() { return Version(1, 12, 2); }
+  static Version getCurrentVersion() { return Version(1, 13, 0); }
 
   /// Return a Version representing the minimum supported VHLO dialect version.
   static Version getMinimumVersion() { return Version(0, 9, 0); }
diff --ruN a/stablehlo/stablehlo/dialect/VhloBytecode.cpp b/stablehlo/stablehlo/dialect/VhloBytecode.cpp
--- stablehlo/stablehlo/dialect/VhloBytecode.cpp
+++ stablehlo/stablehlo/dialect/VhloBytecode.cpp
@@ -387,6 +387,10 @@
   ///     storageTypeMax: svarint
   ///   }
   kUniformQuantizedPerAxisV1Type = 30,
+
+  ///   RankedBufferV1Type {
+  ///   }
+  kRankedBufferV1Type = 41,
 
   /// NoneV1Type {
   /// }
@@ -500,6 +504,8 @@
       DialectBytecodeReader &reader) const;
   UnrankedTensorV1Type readUnrankedTensorV1Type(
       DialectBytecodeReader &reader) const;
+  RankedBufferV1Type readRankedBufferV1Type(
+      DialectBytecodeReader& reader) const;
 
   // TO ADD TYPE: Include a write method for each type in VHLO
   // Ex: void write(SomeType attr, DialectBytecodeWriter &writer) const;
@@ -512,6 +518,7 @@
              DialectBytecodeWriter &writer) const;
   void write(UniformQuantizedV1Type type, DialectBytecodeWriter &writer) const;
   void write(UnrankedTensorV1Type type, DialectBytecodeWriter &writer) const;
+  void write(RankedBufferV1Type type, DialectBytecodeWriter& writer) const;
 };
 
 //===----------------------------------------------------------------------===//
@@ -1090,6 +1097,8 @@
       return readUnrankedTensorV1Type(reader);
     case vhlo_encoding::kWitnessV1Type:
       return WitnessV1Type::get(getContext());
+    case vhlo_encoding::kRankedBufferV1Type:
+      return readRankedBufferV1Type(reader);
     default:
       reader.emitError() << "unknown vhlo type code: " << code;
       return Type();
@@ -1102,7 +1111,7 @@
   return TypeSwitch<Type, LogicalResult>(type)
       .Case<ComplexV1Type, FunctionV1Type, RankedTensorV1Type, TokenV1Type,
             TupleV1Type, UnrankedTensorV1Type, UniformQuantizedPerAxisV1Type,
-            UniformQuantizedV1Type>([&](auto type) {
+            UniformQuantizedV1Type, RankedBufferV1Type>([&](auto type) {
         LOG_WRITE_CALL;
         return write(type, writer), success();
       })
@@ -1475,6 +1484,29 @@
 }
 
 //===----------------------------------------------------------------------===//
+// RankedBufferV1Type
+//===----------------------------------------------------------------------===//
+
+RankedBufferV1Type VhloBytecodeInterface::readRankedBufferV1Type(
+    DialectBytecodeReader& reader) const {
+  LOG_READ_CALL;
+  SmallVector<int64_t> shape;
+  Type elementType;
+  if (failed(reader.readSignedVarInts(shape)) ||
+      failed(reader.readType(elementType)))
+    return RankedBufferV1Type();
+
+  return RankedBufferV1Type::get(getContext(), shape, elementType);
+}
+
+void VhloBytecodeInterface::write(RankedBufferV1Type type,
+                                  DialectBytecodeWriter& writer) const {
+  writer.writeVarInt(vhlo_encoding::kRankedBufferV1Type);
+  writer.writeSignedVarInts(type.getShape());
+  writer.writeType(type.getElementType());
+}
+
+//===----------------------------------------------------------------------===//
 // ResultAccuracyModeAttr
 
 ResultAccuracyModeV1Attr VhloBytecodeInterface::readResultAccuracyModeV1Attr(
diff --ruN a/stablehlo/stablehlo/dialect/VhloDialect.td b/stablehlo/stablehlo/dialect/VhloDialect.td
--- stablehlo/stablehlo/dialect/VhloDialect.td
+++ stablehlo/stablehlo/dialect/VhloDialect.td
@@ -51,6 +51,7 @@
       1.10.0: Add `ResultAccuracy` attribute to `cbrt`, `cosine`, `exponential`, `exponential_minus_one`, `log`, `log_plus_one`, `logistic`, `rsqrt`, `sine`, `sqrt`, `tan` and `tanh` ops.
       1.11.0: Allow (de)serializing VHLO programs mixed with potentially unstable dialects.
       1.12.0: Add `source_target_pairs` attribute to `send` and `recv` ops.
+      1.13.0: Extend `custom_call` op to support `buffer` types.
   }];
 
   let useDefaultAttributePrinterParser = 0;
diff --ruN a/stablehlo/stablehlo/dialect/VhloTypes.cpp b/stablehlo/stablehlo/dialect/VhloTypes.cpp
--- stablehlo/stablehlo/dialect/VhloTypes.cpp
+++ stablehlo/stablehlo/dialect/VhloTypes.cpp
@@ -179,6 +179,12 @@
   addConversion([&](shape::WitnessType type) -> Type {
     return vhlo::WitnessV1Type::get(type.getContext());
   });
+  addConversion([&](MemRefType type) -> Type {
+    auto convertedElementType = convertType(type.getElementType());
+    if (!convertedElementType) return {};
+    return RankedBufferV1Type::get(type.getContext(), type.getShape(),
+                                   convertedElementType);
+  });
 }
 
 void VhloTypeConverter::addVhloToBuiltinConversions() {
@@ -323,6 +329,11 @@
   addConversion([&](WitnessV1Type type) -> Type {
     return shape::WitnessType::get(type.getContext());
   });
+  addConversion([&](RankedBufferV1Type type) -> Type {
+    auto convertedElementType = convertType(type.getElementType());
+    if (!convertedElementType) return {};
+    return MemRefType::get(type.getShape(), convertedElementType);
+  });
 }
 
 namespace {
diff --ruN a/stablehlo/stablehlo/dialect/VhloTypes.td b/stablehlo/stablehlo/dialect/VhloTypes.td
--- stablehlo/stablehlo/dialect/VhloTypes.td
+++ stablehlo/stablehlo/dialect/VhloTypes.td
@@ -326,4 +326,22 @@
 // and we're planning to look into it as part of the work on the dynamism RFC.
 def VHLO_WitnessV1 : VHLO_TypeDef<"WitnessV1", "witness_v1", "0.9.0", "current">;
 
+def VHLO_RankedBufferV1 : VHLO_TypeDef<"RankedBufferV1", "buffer_v1", "1.13.0", "current"> {
+  let parameters = (ins
+    VHLO_Dims:$shape,
+    "::mlir::Type":$elementType/*,
+    "::mlir::Attribute":$encoding*/
+  );
+  let genVerifyDecl = 1;
+  let extraClassDefinition = [{
+    LogicalResult RankedBufferV1Type::verify(
+        llvm::function_ref<mlir::InFlightDiagnostic ()> errFn,
+        ArrayRef<int64_t> shape, ::mlir::Type elementType) {
+      if (!isFromVhlo(elementType)) return errFn() << "expected VHLO types";
+      return success();
+    }
+  }];
+  let assemblyFormat = "`<` custom<Shape>($shape) `` $elementType `>`";
+}
+
 #endif  // STABLEHLO_DIALECT_VHLO_TYPES
diff --ruN a/stablehlo/stablehlo/integrations/python/tests/stablehlo.py b/stablehlo/stablehlo/integrations/python/tests/stablehlo.py
--- stablehlo/stablehlo/integrations/python/tests/stablehlo.py
+++ stablehlo/stablehlo/integrations/python/tests/stablehlo.py
@@ -21,6 +21,9 @@
 import os
 import re
 import tempfile
+
+from jax.interpreters import mlir
+import jax.numpy as jnp
 from mlir import ir
 from mlir import passmanager as pm
 from mlir.dialects import stablehlo
@@ -432,23 +435,43 @@
   assert attr.ulps == 2
 
 
-@run
-def test_reference_api_with_probe():
-  """Tests that probe files are created in the specified directory."""
+def _run_probe_test(tensor_type, arg):
+  """Helper to run a probe test and verify output files."""
   test_tmpdir_base = os.environ.get("TEST_TMPDIR")
   with tempfile.TemporaryDirectory(dir=test_tmpdir_base) as tmpdir:
-    tensor_type = "f32"
-    arg = np.asarray(2, np.float32)
     m = ir.Module.parse(_ASM_FORMAT_WITH_PROBE.format(tensor_type))
-    args = [ir.DenseElementsAttr.get(arg)]
+
+    # bfloat16 requires special handling for DenseElementsAttr creation
+    if arg.dtype == jnp.bfloat16:
+      element_type = mlir.dtype_to_ir_type(arg.dtype)
+      shaped_type = ir.RankedTensorType.get(
+          arg.shape, element_type, loc=ir.Location.unknown(context=ir.Context())
+      )
+      args = [ir.DenseElementsAttr.get(arg, type=shaped_type)]
+    else:
+      args = [ir.DenseElementsAttr.get(arg)]
 
     # Call eval_module, directing probe outputs to the temporary directory.
     stablehlo.eval_module(m, args, probe_instrumentation_dir=tmpdir)
 
-    # Verify that the expected probe files were created.
+    # Verify that the expected probe files were created. The interpreter names
+    # probe files probe1.npy, probe2.npy, etc. The `probe_id` is used as
+    # metadata in `index.csv`.
     probe_file = os.path.join(tmpdir, "probe1.npy")
     metadata_file = os.path.join(tmpdir, "index.csv")
     assert os.path.exists(probe_file), f"Probe file not found: {probe_file}"
     assert os.path.exists(
         metadata_file
     ), f"Metadata file not found: {metadata_file}"
+
+
+@run
+def test_reference_api_with_probe():
+  """Tests that probe files are created in the specified directory."""
+  _run_probe_test("f32", np.asarray(2, np.float32))
+
+
+@run
+def test_reference_api_with_probe_bf16():
+  """Tests that probe files are created for bf16 tensors."""
+  _run_probe_test("bf16", np.asarray(2, jnp.bfloat16))
diff --ruN a/stablehlo/stablehlo/reference/InterpreterInstrumentWithProbe.cpp b/stablehlo/stablehlo/reference/InterpreterInstrumentWithProbe.cpp
--- stablehlo/stablehlo/reference/InterpreterInstrumentWithProbe.cpp
+++ stablehlo/stablehlo/reference/InterpreterInstrumentWithProbe.cpp
@@ -43,7 +43,7 @@
   InterpreterInstrumentWithProbePass(
       const InterpreterInstrumentWithProbePassOptions& opts)
       : InterpreterInstrumentWithProbePassBase<
-            InterpreterInstrumentWithProbePass>(opts){};
+            InterpreterInstrumentWithProbePass>(opts) {};
   void runOnOperation() override;
 
  private:
@@ -122,7 +122,16 @@
 }
 
 bool InterpreterInstrumentWithProbePass::shouldProbeValue(Value value) const {
-  return isa<TensorType>(value.getType());
+  // Check if the value's type is a RankedTensorType.
+  auto tensorType = dyn_cast<RankedTensorType>(value.getType());
+  if (!tensorType) return false;
+
+  // Check if the RankedTensorType has a static shape.
+  if (!tensorType.hasStaticShape()) return false;
+
+  Type elementType = tensorType.getElementType();
+
+  return elementType.isIntOrFloat() || isa<ComplexType>(elementType);
 }
 
 }  // namespace
diff --ruN a/stablehlo/stablehlo/reference/NumPy.cpp b/stablehlo/stablehlo/reference/NumPy.cpp
--- stablehlo/stablehlo/reference/NumPy.cpp
+++ stablehlo/stablehlo/reference/NumPy.cpp
@@ -319,6 +319,7 @@
   if (type.isF16()) return Functor<uint16_t>()(std::forward<Args>(args)...);
   if (type.isF32()) return Functor<float>()(std::forward<Args>(args)...);
   if (type.isF64()) return Functor<double>()(std::forward<Args>(args)...);
+  if (type.isBF16()) return Functor<uint16_t>()(std::forward<Args>(args)...);
   if (auto complexTy = dyn_cast<ComplexType>(type)) {
     auto complexElemTy = complexTy.getElementType();
 
diff --ruN a/stablehlo/stablehlo/tests/interpret/probe.mlir b/stablehlo/stablehlo/tests/interpret/probe.mlir
--- stablehlo/stablehlo/tests/interpret/probe.mlir
+++ stablehlo/stablehlo/tests/interpret/probe.mlir
@@ -43,6 +43,17 @@
   %2 = stablehlo.add %0, %1 : tensor<3xf64>
   %3 = interpreter.probe %2, probe_id = "probe_f64" : tensor<3xf64>
   check.expect_serialized_eq %3, probe_id = "probe_f64" : tensor<3xf64>
+  func.return
+}
+
+// -----
+
+func.func @probe_bf16() {
+  %0 = stablehlo.constant dense<[1.0, 2.5, -3.0]> : tensor<3xbf16>
+  %1 = stablehlo.constant dense<[0.5, 1.5, 0.0]> : tensor<3xbf16>
+  %2 = stablehlo.add %0, %1 : tensor<3xbf16>
+  %3 = interpreter.probe %2, probe_id = "probe_bf16" : tensor<3xbf16>
+  check.expect_serialized_eq %3, probe_id = "probe_bf16" : tensor<3xbf16>
   func.return
 }
 
diff --ruN a/stablehlo/stablehlo/tests/ops_stablehlo.mlir b/stablehlo/stablehlo/tests/ops_stablehlo.mlir
--- stablehlo/stablehlo/tests/ops_stablehlo.mlir
+++ stablehlo/stablehlo/tests/ops_stablehlo.mlir
@@ -6639,3 +6639,184 @@
   } : (!stablehlo.token) -> tensor<f32>
   func.return
 }
+
+// -----
+
+func.func @custom_call_op_with_buffer_type(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Pin",
+    api_version = 4 : i32
+  } : (tensor<2xf32>) -> memref<2xf32>
+  %1 = "stablehlo.custom_call"(%0) {
+    call_target_name = "foo",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [],
+        operand_index = 0,
+        operand_tuple_indices = []>]
+  } : (memref<2xf32>) -> memref<2xf32>
+  %2 = "stablehlo.custom_call"(%1) {
+    call_target_name = "Unpin",
+    api_version = 4 : i32
+  } : (memref<2xf32>) -> tensor<2xf32>
+  func.return %2 : tensor<2xf32>
+}
+
+func.func @custom_call_op_create_buffer_type() -> memref<2xf32> {
+  %0 = "stablehlo.custom_call"() {
+    call_target_name = "CreateBuffer",
+    api_version = 4 : i32
+  } : () -> memref<2xf32>
+  func.return %0 : memref<2xf32>
+}
+
+func.func @tuple_op_with_buffer_type(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>> {
+  %0 = "stablehlo.tuple"(%arg0, %arg1) : (tensor<2xf32>, memref<2xf32>) -> tuple<tensor<2xf32>, memref<2xf32>>
+  func.return %0 : tuple<tensor<2xf32>, memref<2xf32>>
+}
+
+func.func @get_tuple_element_op_with_buffer_type(%arg0: tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32> {
+  %0 = "stablehlo.get_tuple_element"(%arg0) {
+    index = 1 : i32
+  } : (tuple<tensor<2xf32>, memref<2xf32>>) -> memref<2xf32>
+  func.return %0 : memref<2xf32>
+}
+
+func.func @while_op_with_buffer_type(%arg0: tensor<i1>, %arg1: memref<2xf32>) -> memref<2xf32> {
+  %0:2 = stablehlo.while(%iterArg0 = %arg0, %iterArg1 = %arg1) : tensor<i1>, memref<2xf32>
+    cond {
+      stablehlo.return %iterArg0 : tensor<i1>
+    } do {
+      %1 = "stablehlo.custom_call"(%iterArg1) {
+        call_target_name = "foo",
+        api_version = 4 : i32,
+        output_operand_aliases = [
+          #stablehlo.output_operand_alias<output_tuple_indices = [],
+            operand_index = 0,
+            operand_tuple_indices = []>]
+      } : (memref<2xf32>) -> memref<2xf32>
+      stablehlo.return %iterArg0, %1 : tensor<i1>, memref<2xf32>
+    }
+  func.return %0#1: memref<2xf32>
+}
+
+func.func @custom_call_op_create_buffer_with_input(%arg0: tensor<2xf32>) -> memref<2xf32> {
+  // expected-error@+1 {{CreateBuffer custom_call shouldn't have any inputs}}
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "CreateBuffer",
+    api_version = 4 : i32
+  } : (tensor<2xf32>) -> memref<2xf32>
+  func.return %0 : memref<2xf32>
+}
+
+func.func @custom_call_op_pin_with_tensor_output(%arg0: tensor<2xf32>) -> tensor<2xf32> {
+  // expected-error@+1 {{Pin custom_call should have a memref output}}
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Pin",
+    api_version = 4 : i32
+  } : (tensor<2xf32>) -> tensor<2xf32>
+  func.return %0 : tensor<2xf32>
+}
+
+func.func @custom_call_op_unpin_with_incompatible_input_output(%arg0: memref<2xf32>) -> tensor<2xi32> {
+  // expected-error@+1 {{Unpin custom_call should have compatible input and output types}}
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Unpin",
+    api_version = 4 : i32
+  } : (memref<2xf32>) -> tensor<2xi32>
+  func.return %0 : tensor<2xi32>
+}
+
+func.func @add_op_with_buffer_type(%arg0: memref<4xf32>, %arg1: memref<4xf32>) -> memref<4xf32> {
+  // expected-error-re@+1 {{operand #0 must be ranked tensor of {{.*}}, but got 'memref<4xf32>'}}
+  %0 = "stablehlo.add"(%arg0, %arg1) : (memref<4xf32>, memref<4xf32>) -> memref<4xf32>
+  func.return %0 : memref<4xf32>
+}
+
+func.func @if_op_with_buffer_type(%pred : tensor<i1>, %branch_operand : memref<2xf32>) -> memref<2xf32> {
+  // expected-error-re@+1 {{result #0 must be variadic of ranked tensor of {{.*}}, but got 'memref<2xf32>'}}
+  %0 = "stablehlo.if"(%pred) ({
+      "stablehlo.return"(%branch_operand) : (memref<2xf32>) -> ()
+    }, {
+      "stablehlo.return"(%branch_operand) : (memref<2xf32>) -> ()
+    }) : (tensor<i1>) -> memref<2xf32>
+  func.return %0 : memref<2xf32>
+}
+
+func.func @custom_call_pin_with_output_operand_alias(%arg0: tensor<2xf32>) -> memref<2xf32> {
+  // expected-error@+1 {{shapes mismatch in the output_operand_alias attribute: operand part has type 'tensor<2xf32>' and output part has type 'memref<2xf32>'}}
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Pin",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [],
+        operand_index = 0,
+        operand_tuple_indices = []>]
+  } : (tensor<2xf32>) -> memref<2xf32>
+  func.return %0 : memref<2xf32>
+}
+
+func.func @custom_call_unpin_with_output_operand_alias(%arg0: memref<2xf32>) -> tensor<2xf32> {
+  // expected-error@+1 {{shapes mismatch in the output_operand_alias attribute: operand part has type 'memref<2xf32>' and output part has type 'tensor<2xf32>'}}
+  %0 = "stablehlo.custom_call"(%arg0) {
+    call_target_name = "Unpin",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [],
+        operand_index = 0,
+        operand_tuple_indices = []>]
+  } : (memref<2xf32>) -> tensor<2xf32>
+  func.return %0 : tensor<2xf32>
+}
+
+func.func @custom_call_buffer_operand_in_multiple_output_operand_aliases(%arg0: tensor<2xf32>, %arg1: memref<2xf32>) -> (memref<2xf32>, memref<2xf32>) {
+  // expected-error@+1 {{buffer operand 1[] is used in output_operand_alias more than once}}
+  %0:2 = "stablehlo.custom_call"(%arg0, %arg1) {
+    call_target_name = "foo",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [0],
+        operand_index = 1,
+        operand_tuple_indices = []>,
+      #stablehlo.output_operand_alias<output_tuple_indices = [1],
+        operand_index = 1,
+        operand_tuple_indices = []>]
+  } : (tensor<2xf32>, memref<2xf32>) -> (memref<2xf32>, memref<2xf32>)
+  func.return %0#0, %0#1 : memref<2xf32>, memref<2xf32>
+}
+
+func.func @custom_call_buffer_output_in_multiple_output_operand_aliases(%arg0: memref<2xf32>, %arg1: memref<2xf32>) -> tuple<tuple<memref<2xf32>>> {
+  %0 = "stablehlo.tuple"(%arg0) : (memref<2xf32>) -> tuple<memref<2xf32>>
+  %1 = "stablehlo.tuple"(%0, %arg1) : (tuple<memref<2xf32>>, memref<2xf32>) -> tuple<tuple<memref<2xf32>>, memref<2xf32>>
+  // expected-error@+1 {{buffer output [0, 0] is used in output_operand_alias more than once}}
+  %2 = "stablehlo.custom_call"(%1) {
+    call_target_name = "foo",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [0, 0],
+        operand_index = 0,
+        operand_tuple_indices = [0, 0]>,
+      #stablehlo.output_operand_alias<output_tuple_indices = [0, 0],
+        operand_index = 0,
+        operand_tuple_indices = [1]>]
+  } : (tuple<tuple<memref<2xf32>>, memref<2xf32>>) -> tuple<tuple<memref<2xf32>>>
+  func.return %2 : tuple<tuple<memref<2xf32>>>
+}
+
+func.func @custom_call_buffer_output_not_in_output_operand_aliases(%arg0: memref<2xf32>, %arg1: memref<2xf32>) -> (memref<2xf32>, memref<2xf32>, memref<2xf32>) {
+  %0 = "stablehlo.tuple"(%arg0) : (memref<2xf32>) -> tuple<memref<2xf32>>
+  %1 = "stablehlo.tuple"(%0, %arg1) : (tuple<memref<2xf32>>, memref<2xf32>) -> tuple<tuple<memref<2xf32>>, memref<2xf32>>
+  // expected-error@+1 {{buffer output [2] is not used in output_operand_alias}}
+  %2:3 = "stablehlo.custom_call"(%1) {
+    call_target_name = "foo",
+    api_version = 4 : i32,
+    output_operand_aliases = [
+      #stablehlo.output_operand_alias<output_tuple_indices = [0],
+        operand_index = 0,
+        operand_tuple_indices = [0, 0]>,
+      #stablehlo.output_operand_alias<output_tuple_indices = [1],
+        operand_index = 0,
+        operand_tuple_indices = [1]>]
+  } : (tuple<tuple<memref<2xf32>>, memref<2xf32>>) -> (memref<2xf32>, memref<2xf32>, memref<2xf32>)
+  func.return %2#0, %2#1, %2#2 : memref<2xf32>, memref<2xf32>, memref<2xf32>
+}
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_aggressive_folder.mlir
@@ -246,6 +246,9 @@
 
 // -----
 
+////////
+// ConvertOp
+
 // CHECK-LABEL: func @eval_convert_f32_to_i64
 func.func @eval_convert_f32_to_i64() -> tensor<2xi64> {
   // CHECK-NOT: stablehlo.convert
@@ -254,6 +257,42 @@
   %0 = stablehlo.constant dense<[1.0, 2.0]> : tensor<2xf32>
   %1 = stablehlo.convert %0 : (tensor<2xf32>) -> tensor<2xi64>
   func.return %1 : tensor<2xi64>
+}
+
+// CHECK-LABEL: func @eval_convert_bool_f32
+func.func @eval_convert_bool_f32() -> tensor<2xf32> {
+  // CHECK-NEXT: [[CST:%.+]] = stablehlo.constant dense<[0.000000e+00, 1.000000e+00]> : tensor<2xf32>
+  %cst = stablehlo.constant dense<[0, 1]> : tensor<2xi1>
+  %0 = stablehlo.convert %cst : (tensor<2xi1>) -> tensor<2xf32>
+  // CHECK-NEXT: return [[CST]]
+  func.return %0 : tensor<2xf32>
+}
+
+// CHECK-LABEL: func @eval_convert_bool_i32
+func.func @eval_convert_bool_i32() -> tensor<2xi32> {
+  // CHECK-NEXT: [[CST:%.+]] = stablehlo.constant dense<[0, 1]> : tensor<2xi32>
+  %cst = stablehlo.constant dense<[0, 1]> : tensor<2xi1>
+  %0 = stablehlo.convert %cst : (tensor<2xi1>) -> tensor<2xi32>
+  // CHECK-NEXT: return [[CST]]
+  func.return %0 : tensor<2xi32>
+}
+
+// CHECK-LABEL: func @eval_convert_i32_bool
+func.func @eval_convert_i32_bool() -> tensor<3xi1> {
+  // CHECK-NEXT: [[CST:%.+]] = stablehlo.constant dense<[false, true, true]> : tensor<3xi1>
+  %cst = stablehlo.constant dense<[0, 1, 10]> : tensor<3xi32>
+  %0 = stablehlo.convert %cst : (tensor<3xi32>) -> tensor<3xi1>
+  // CHECK-NEXT: return [[CST]]
+  func.return %0 : tensor<3xi1>
+}
+
+// CHECK-LABEL: func @eval_convert_f32_bool
+func.func @eval_convert_f32_bool() -> tensor<4xi1> {
+  // CHECK-NEXT: [[CST:%.+]] = stablehlo.constant dense<[true, false, true, true]> : tensor<4xi1>
+  %cst = stablehlo.constant dense<[-1.0, 0.0, 1.0, 10.0]> : tensor<4xf32>
+  %0 = stablehlo.convert %cst : (tensor<4xf32>) -> tensor<4xi1>
+  // CHECK-NEXT: return [[CST]]
+  func.return %0 : tensor<4xi1>
 }
 
 // -----
@@ -318,6 +357,9 @@
 
 // -----
 
+////////
+// SqrtOp
+
 // CHECK-LABEL: func @fold_sqrt
 func.func @fold_sqrt() -> (tensor<f32>) {
   // CHECK: [[RESULT0:%.*]] = stablehlo.constant dense<2.0{{.*}}> : tensor<f32>
@@ -327,7 +369,37 @@
   func.return %1 : tensor<f32>
 }
 
-// -----
+//
+
+////////
+// SetDimensionSizeOp
+
+// CHECK-LABEL: func.func @fold_set_dimension_size
+// CHECK-SAME:   ([[ARG0:%.+]]: tensor<10xf32>)
+func.func @fold_set_dimension_size(%arg0: tensor<10xf32>) -> tensor<10xf32> {
+  // CHECK-NOT: stablehlo.set_dimension_size
+  // CHECK: return [[ARG0]]
+  %c = stablehlo.constant dense<10> : tensor<i32>
+  %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<10xf32>
+  return %0 : tensor<10xf32>
+}
+
+// -----
+
+// Don't fold when set_dimension_size result is bounded.
+// CHECK-LABEL: func.func @no_fold_set_dimension_size
+func.func @no_fold_set_dimension_size(%arg0: tensor<10xf32>) -> tensor<?xf32, #stablehlo.bounds<10>> {
+  %c = stablehlo.constant dense<10> : tensor<i32>
+  // CHECK: [[RESULT0:%.+]] = stablehlo.set_dimension_size
+  // CHECK-NEXT: return [[RESULT0]]
+  %0 = stablehlo.set_dimension_size %arg0, %c, dim = 0 : (tensor<10xf32>, tensor<i32>) -> tensor<?xf32, #stablehlo.bounds<10>>
+  return %0 : tensor<?xf32, #stablehlo.bounds<10>>
+}
+
+// -----
+
+////////
+// TransposeOp
 
 // CHECK-LABEL: func @eval_transpose
 func.func @eval_transpose() -> (tensor<2x3x2xi32>, tensor<2x4x3xi32>, tensor<4x3x2xi32>) {
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_probe_instrumentation.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_probe_instrumentation.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_probe_instrumentation.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_probe_instrumentation.mlir
@@ -1,4 +1,4 @@
-// RUN: stablehlo-opt --interpreter-instrument-with-probe="useDebugInfo=true" --split-input-file --verify-diagnostics %s | FileCheck %s
+// RUN: stablehlo-opt --allow-unregistered-dialect --interpreter-instrument-with-probe="useDebugInfo=true" --split-input-file --verify-diagnostics %s | FileCheck %s
 
 // CHECK-LABEL: func @instrument_basic_no_location
 func.func @instrument_basic_no_location(%arg0: tensor<1x2xi32>, %arg1: tensor<1x2xi32>) -> tensor<1x2xi32> {
@@ -98,3 +98,14 @@
 
   func.return %results1 : tensor<i64>
 }
+
+// -----
+
+// CHECK-LABEL: func @test_string_type
+func.func @test_string_type() -> tensor<!tf_type.string> {
+  // CHECK: "tf.Const"
+  // CHECK-NOT: interpreter.probe
+  // CHECK-NEXT: return
+  %0 = "tf.Const"() {value = dense<"hello"> : tensor<!tf_type.string>} : () -> tensor<!tf_type.string>
+  return %0 : tensor<!tf_type.string>
+}
diff --ruN a/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir b/stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
--- stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
+++ stablehlo/stablehlo/tests/transforms/stablehlo_refine_shapes.mlir
@@ -19,7 +19,8 @@
 
 // -----
 
-// expected-error@-3{{must have no more than one function or a `main` function to clearly identify which function will be refined}}
+// expected-error@+1{{must have no more than one function or a `main` function to clearly identify which function will be refined}}
+module {
 func.func @error_too_many_functions(%arg0: tensor<f32>) -> tensor<f32> {
   %0 = func.call @helper(%arg0) : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
@@ -27,6 +28,7 @@
 
 func.func private @helper(%arg0: tensor<f32>) -> tensor<f32> {
   return %arg0 : tensor<f32>
+}
 }
 
 // -----
diff --ruN a/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir b/stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
--- stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
+++ stablehlo/stablehlo/tests/vhlo/stablehlo_legalize_to_vhlo.mlir
@@ -2971,6 +2971,13 @@
   return %0 : tuple<!stablehlo.token>
 }
 
+// CHECK-LABEL: type_buffer
+// CHECK-NEXT: (%[[ARG0:.*]]: !vhlo.buffer_v1<2x!vhlo.f32_v1>)
+func.func @type_buffer(%arg0: memref<2xf32>) -> memref<2xf32> {
+  // CHECK: "vhlo.return_v1"(%[[ARG0]]) : (!vhlo.buffer_v1<2x!vhlo.f32_v1>) -> ()
+  func.return %arg0 : memref<2xf32>
+}
+
 // ============ DEPENDENCIES  ============
 
 func.func @composite_target(%arg0: tensor<f32>) -> tensor<f32> {
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_16_0.mlir
@@ -1,6 +1,7 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=0.16.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @reduce_with_promotable_types(%arg0: tensor<4x4xf32>, %arg1 : tensor<f32>)
     -> (tensor<4xf64>) {
 
@@ -15,10 +16,12 @@
 
   func.return %0: tensor<4xf64>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @all_reduce_with_promotable_types(%operand: tensor<f32>) -> tensor<f64> {
 
   // expected-error @+1 {{failed to legalize operation 'vhlo.all_reduce_v2' that was explicitly marked illegal}}
@@ -33,10 +36,12 @@
 
   func.return %result : tensor<f64>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @reduce_scatter_with_promotable_types(%data: tensor<4x16xf32>) -> tensor<4x4xf64> {
 
   // expected-error @+1 {{failed to legalize operation 'vhlo.reduce_scatter_v1' that was explicitly marked illegal}}
@@ -50,10 +55,12 @@
       use_global_device_ids} : (tensor<4x16xf32>) -> tensor<4x4xf64>
   func.return %0 : tensor<4x4xf64>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @reduce_window_with_promotable_types(%arg0: tensor<4x2xf32>,
     %arg1: tensor<4x2xf32>, %init0: tensor<f32>, %init1: tensor<f32>) ->
     (tensor<2x2xf64>, tensor<2x2xf32>) {
@@ -73,10 +80,12 @@
               (tensor<2x2xf64>, tensor<2x2xf32>)
   func.return %0#0, %0#1 : tensor<2x2xf64>, tensor<2x2xf32>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @scatter_with_promotable_types(%input_tensor: tensor<200x100x300xf32>,
     %scatter_indices: tensor<10x2xi32>, %updates: tensor<10x300xf32>) ->
       tensor<200x100x300xf64> {
@@ -99,10 +108,12 @@
       tensor<200x100x300xf64>
   func.return %0 : tensor<200x100x300xf64>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.16.0}}
+// expected-error @+1 {{failed to convert VHLO to v0.16.0}}
+module {
 func.func @select_and_scatter_with_promotable_types(
     %arg0: tensor<10x24x24x64xf32>,
     %arg1: tensor<10x12x12x64xf32>) -> () {
@@ -127,3 +138,4 @@
         tensor<10x24x24x64xf64>
   func.return
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_9_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_9_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_9_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.0_9_0.mlir
@@ -1,17 +1,21 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=0.9.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v0.9.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v0.9.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_fp8_E5M2FNUZ(%arg0: tensor<f8E5M2FNUZ>) -> tensor<f8E5M2FNUZ> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<f8E5M2FNUZ>
   func.return %0 : tensor<f8E5M2FNUZ>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v0.9.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v0.9.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_fp8_E4M3FNUZ(%arg0: tensor<f8E4M3FNUZ>) -> tensor<f8E4M3FNUZ> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<f8E4M3FNUZ>
   func.return %0 : tensor<f8E4M3FNUZ>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_11_0.mlir
@@ -1,6 +1,7 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.11.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.11.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.11.0}}
+module {
 func.func public @send_op(%arg0: tensor<f32>, %arg1: !stablehlo.token) -> !stablehlo.token {
   // expected-error @+1 {{failed to legalize operation 'vhlo.send_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.send"(%arg0, %arg1) {
@@ -10,10 +11,12 @@
   } : (tensor<f32>, !stablehlo.token) -> !stablehlo.token
   func.return %0 : !stablehlo.token
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.11.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.11.0}}
+module {
 func.func public @recv_op(%arg0: !stablehlo.token) -> (tensor<f32>, !stablehlo.token) {
   // expected-error @+1 {{failed to legalize operation 'vhlo.recv_v2' that was explicitly marked illegal}}
   %0:2 = "stablehlo.recv"(%arg0) {
@@ -23,3 +26,4 @@
   } : (!stablehlo.token) -> (tensor<f32>, !stablehlo.token)
   func.return %0#0, %0#1 : tensor<f32>, !stablehlo.token
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_1_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_1_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_1_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_1_0.mlir
@@ -1,17 +1,21 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.1.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.1.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.1.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_i2(%arg0: tensor<i2>) -> tensor<i2> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<i2>
   func.return %0 : tensor<i2>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.1.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.1.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_ui2(%arg0: tensor<ui2>) -> tensor<ui2> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<ui2>
   func.return %0 : tensor<ui2>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_2_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_2_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_2_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_2_0.mlir
@@ -1,6 +1,7 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.2.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.2.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.2.0}}
+module {
 func.func @custom_call_dictionary_attr(%arg0: tensor<f32>) -> tensor<f32> {
 // expected-error @+1 {{failed to legalize operation 'vhlo.custom_call_v1' that was explicitly marked illegal}}
 %0 = "stablehlo.custom_call"(%arg0) {
@@ -10,10 +11,12 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.2.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.2.0}}
+module {
 func.func @custom_call_dictionary_attr(%arg0: tensor<f32>) -> tensor<f32> {
 // expected-error @+1 {{failed to legalize operation 'vhlo.custom_call_v1' that was explicitly marked illegal}}
 %0 = "stablehlo.custom_call"(%arg0) {
@@ -22,3 +25,4 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_4_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_4_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_4_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_4_0.mlir
@@ -1,6 +1,7 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.4.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.4.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.4.0}}
+module {
 func.func @all_reduce_variadic(%arg0: tensor<f32>, %arg1: tensor<f32>) -> (tensor<f32>, tensor<f32>) {
   // expected-error @+1 {{failed to legalize operation 'vhlo.all_reduce_v2' that was explicitly marked illegal}}
   %0:2 = "stablehlo.all_reduce"(%arg0, %arg1) ({
@@ -12,10 +13,12 @@
   } : (tensor<f32>, tensor<f32>) -> (tensor<f32>, tensor<f32>)
   func.return %0#0, %0#1 : tensor<f32>, tensor<f32>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.4.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.4.0}}
+module {
 func.func @all_gather_variadic(%arg0: tensor<16x8xf32>, %arg1: tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>) {
   // expected-error @+1 {{failed to legalize operation 'vhlo.all_gather_v2' that was explicitly marked illegal}}
   %0:2 = "stablehlo.all_gather"(%arg0, %arg1) {
@@ -24,10 +27,12 @@
   } : (tensor<16x8xf32>, tensor<16x8xf32>) -> (tensor<16x16xf32>, tensor<16x16xf32>)
   func.return %0#0, %0#1 : tensor<16x16xf32>, tensor<16x16xf32>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.4.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.4.0}}
+module {
 func.func @all_to_all_variadic(%arg0: tensor<4x16xf32>, %arg1: tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>) {
   // expected-error @+1 {{failed to legalize operation 'vhlo.all_to_all_v2' that was explicitly marked illegal}}
   %0:2 = "stablehlo.all_to_all"(%arg0, %arg1) {
@@ -39,3 +44,4 @@
   } : (tensor<4x16xf32>, tensor<5x16xf32>) -> (tensor<16x4xf32>, tensor<20x4xf32>)
   func.return %0#0, %0#1 : tensor<16x4xf32>, tensor<20x4xf32>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_5_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_5_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_5_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_5_0.mlir
@@ -1,6 +1,7 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.5.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.5.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.5.0}}
+module {
 func.func @dot_general_algorithm(%arg0: tensor<2x2x2xi64>, %arg1: tensor<2x2x2xi64>) -> tensor<2x2x2xi64> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.dot_general_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.dot_general"(%arg0, %arg1) <{
@@ -9,19 +10,24 @@
     algorithm = #stablehlo.dot_algorithm<lhs_precision_type = tf32, rhs_precision_type = tf32, accumulation_type = f32, lhs_component_count = 1, rhs_component_count = 1, num_primitive_operations = 1, allow_imprecise_accumulation = false>
   }> : (tensor<2x2x2xi64>, tensor<2x2x2xi64>) -> tensor<2x2x2xi64>  return %0 : tensor<2x2x2xi64>
 }
-
-// -----
-
-// expected-error @-3 {{failed to convert VHLO to v1.5.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
-func.func @none_type() attributes {stablehlo.attr = none } {
-  return
 }
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.5.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.5.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
+func.func @none_type() attributes {stablehlo.attr = none } {
+  return
+}
+}
+
+// -----
+
+// expected-error @+2 {{failed to convert VHLO to v1.5.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @tf32_type() attributes {stablehlo.attr = tf32 } {
   return
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_6_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_6_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_6_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_6_0.mlir
@@ -1,17 +1,21 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.6.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.6.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.6.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f8E4M3(%arg0: tensor<f8E4M3>) -> tensor<f8E4M3> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<f8E4M3>
   func.return %0 : tensor<f8E4M3>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.6.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.6.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f8E3M4(%arg0: tensor<f8E3M4>) -> tensor<f8E3M4> {
   %0 = stablehlo.add %arg0, %arg0 : tensor<f8E3M4>
   func.return %0 : tensor<f8E3M4>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_7_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_7_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_7_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_7_0.mlir
@@ -1,35 +1,43 @@
 // RUN: stablehlo-opt --stablehlo-legalize-to-vhlo --vhlo-to-version='target=1.7.0' --verify-diagnostics --split-input-file %s
 
-// expected-error @-3 {{failed to convert VHLO to v1.7.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.7.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f4E2M1FN(%arg0: tensor<f4E2M1FN>, %arg1: tensor<f4E2M1FN>) -> tensor<f4E2M1FN> {
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f4E2M1FN>, tensor<f4E2M1FN>) -> tensor<f4E2M1FN>
   func.return %0 : tensor<f4E2M1FN>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.7.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.7.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f6E2M3FN(%arg0: tensor<f6E2M3FN>, %arg1: tensor<f6E2M3FN>) -> tensor<f6E2M3FN> {
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E2M3FN>, tensor<f6E2M3FN>) -> tensor<f6E2M3FN>
   func.return %0 : tensor<f6E2M3FN>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.7.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.7.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f6E3M2FN(%arg0: tensor<f6E3M2FN>, %arg1: tensor<f6E3M2FN>) -> tensor<f6E3M2FN> {
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f6E3M2FN>, tensor<f6E3M2FN>) -> tensor<f6E3M2FN>
   func.return %0 : tensor<f6E3M2FN>
 }
+}
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.7.0}}
-// expected-error @+1 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+// expected-error @+2 {{failed to convert VHLO to v1.7.0}}
+// expected-error @+2 {{failed to legalize operation 'vhlo.func_v1' that was explicitly marked illegal}}
+module {
 func.func @type_f8E8M0FNU(%arg0: tensor<f8E8M0FNU>, %arg1: tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU> {
   %0 = "stablehlo.add"(%arg0, %arg1) : (tensor<f8E8M0FNU>, tensor<f8E8M0FNU>) -> tensor<f8E8M0FNU>
   func.return %0 : tensor<f8E8M0FNU>
 }
+}
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_8_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_8_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_8_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_8_0.mlir
@@ -11,7 +11,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.8.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.8.0}}
+module {
 func.func @attr_result_accuracy_highest(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.exponential_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.exponential"(%arg0) {
@@ -19,4 +20,5 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
diff --ruN a/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_9_0.mlir b/stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_9_0.mlir
--- stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_9_0.mlir
+++ stablehlo/stablehlo/tests/vhlo/vhlo_to_version_downgrade_invalid.1_9_0.mlir
@@ -11,7 +11,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @cbrt_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.cbrt_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.cbrt"(%arg0) {
@@ -19,6 +20,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -32,7 +34,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @cosine_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.cosine_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.cosine"(%arg0) {
@@ -40,6 +43,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -53,7 +57,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @exponential_minus_one_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.exponential_minus_one_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.exponential_minus_one"(%arg0) {
@@ -61,6 +66,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -74,7 +80,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @log_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.log_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.log"(%arg0) {
@@ -82,6 +89,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -95,7 +103,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @log_plus_one_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.log_plus_one_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.log_plus_one"(%arg0) {
@@ -103,6 +112,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -116,7 +126,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @logistic_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.logistic_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.logistic"(%arg0) {
@@ -124,6 +135,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -137,7 +149,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @rsqrt_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.rsqrt_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.rsqrt"(%arg0) {
@@ -145,6 +158,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -158,7 +172,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @sine_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.sine_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.sine"(%arg0) {
@@ -166,6 +181,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -179,7 +195,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @sqrt_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.sqrt_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.sqrt"(%arg0) {
@@ -187,6 +204,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -200,7 +218,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @tan_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.tan_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.tan"(%arg0) {
@@ -208,6 +227,7 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
+}
 
 // -----
 
@@ -221,7 +241,8 @@
 
 // -----
 
-// expected-error @-3 {{failed to convert VHLO to v1.9.0}}
+// expected-error @+1 {{failed to convert VHLO to v1.9.0}}
+module {
 func.func @tanh_invalid(%arg0: tensor<f32>) -> tensor<f32> {
   // expected-error @+1 {{failed to legalize operation 'vhlo.tanh_v2' that was explicitly marked illegal}}
   %0 = "stablehlo.tanh"(%arg0) {
@@ -229,4 +250,5 @@
   } : (tensor<f32>) -> tensor<f32>
   func.return %0 : tensor<f32>
 }
-
+}
+
diff --ruN a/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp b/stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
--- stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
+++ stablehlo/stablehlo/transforms/ChloLegalizeToStablehlo.cpp
@@ -6,6 +6,11 @@
 
 // Implements logic for lowering CHLO ops to StableHLO and Shape dialect ops,
 // taking care of CHLO's broadcasting semantics
+
+// This must preceed all other headers, otherwise during Windows cross
+// compilation, M_PI will not be defined.
+#define _USE_MATH_DEFINES
+
 #include <algorithm>
 #include <array>
 #include <cassert>
diff --ruN a/stablehlo/stablehlo/transforms/VhloToVersion.cpp b/stablehlo/stablehlo/transforms/VhloToVersion.cpp
--- stablehlo/stablehlo/transforms/VhloToVersion.cpp
+++ stablehlo/stablehlo/transforms/VhloToVersion.cpp
@@ -199,6 +199,10 @@
   if (auto unranked = dyn_cast<UnrankedTensorV1Type>(type))
     return isLegalType(unranked.getElementType(), targetVersion);
 
+  if (auto buffer = dyn_cast<RankedBufferV1Type>(type)) {
+    return isLegalType(buffer.getElementType(), targetVersion);
+  }
+
   // Is VHLO and valid version, success.
   return success();
 }
diff --ruN a/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp b/stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
--- stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
+++ stablehlo/stablehlo/transforms/optimization/StablehloAggressiveFolder.cpp
@@ -200,8 +200,11 @@
   auto newType = getElementTypeOrSelf(resultType);
   size_t newBitWidth = newType.getIntOrFloatBitWidth();
 
-  bool isOldTypeUnsigned = oldType.isInteger(1) || oldType.isUnsignedInteger();
-  bool isNewTypeUnsigned = newType.isInteger(1) || newType.isUnsignedInteger();
+  bool isOldTypeUnsigned =
+      oldType.isSignlessInteger(1) || oldType.isUnsignedInteger();
+  bool isNewTypeUnsigned =
+      newType.isSignlessInteger(1) || newType.isUnsignedInteger();
+  bool isNewTypeBoolean = newType.isSignlessInteger(1);
 
   if (isa<FloatType>(oldType)) {
     if (auto newFloatType = dyn_cast<FloatType>(newType)) {
@@ -217,6 +220,16 @@
                                           llvm::RoundingMode::NearestTiesToEven,
                                           &losesInfo);
             return newValue;
+          });
+    }
+
+    // Float -> Boolean
+    if (isNewTypeBoolean) {
+      return foldConvertHelper<FloatAttr, IntegerAttr>(
+          rewriter, op, elements, resultType,
+          [newBitWidth](const APFloat& operand, bool& /*castStatus*/) {
+            APInt resVal(1, operand.isZero() ? 0 : 1);
+            return resVal.sextOrTrunc(newBitWidth);
           });
     }
 
@@ -249,6 +262,16 @@
           apf.convertFromAPInt(operand, !isOldTypeUnsigned,
                                APFloat::rmNearestTiesToEven);
           return apf;
+        });
+  }
+
+  // Int -> Boolean
+  if (isNewTypeBoolean) {
+    return foldConvertHelper<IntegerAttr, IntegerAttr>(
+        rewriter, op, elements, resultType,
+        [newBitWidth](const APInt& operand, bool& /*castStatus*/) {
+          APInt resVal(1, operand.isZero() ? 0 : 1);
+          return resVal.sextOrTrunc(newBitWidth);
         });
   }
 
@@ -813,6 +836,36 @@
   };
 };
 
+// Pattern: set_dim_size(op, size, dim) -> op [op[dim] == size]
+struct FoldSetDimensionSizeOpPattern
+    : public ShapeOpRewritePattern<SetDimensionSizeOp> {
+  using ShapeOpRewritePattern::ShapeOpRewritePattern;
+
+  LogicalResult matchAndRewrite(SetDimensionSizeOp op,
+                                PatternRewriter& rewriter) const override {
+    auto resultType = op.getType();
+    // No need to verify static shape or dtype here since we aren't evaluating
+    // dtype, just folding set_dim_size ops with no semantic meaning.
+
+    SplatElementsAttr cstSplatAttr;
+    matchPattern(op.getSize(), m_Constant(&cstSplatAttr));
+    if (!cstSplatAttr)
+      return rewriter.notifyMatchFailure(op, "size operand not constant");
+
+    // Compare to the dimension size, not the bound size.
+    // We can't fold and change shapes, but we do want to fold meaningless
+    // set_dim_size ops that use a constant to set a static dimension size
+    // like `set_dim_size(X, 2, dim=0) : (tensor<2xf32>) -> tensor<2xf32>`
+    if (cstSplatAttr.getSplatValue<APInt>() !=
+        resultType.getDimSize(op.getDimension()))
+      return rewriter.notifyMatchFailure(op,
+                                         "dim size does not match result type");
+
+    rewriter.replaceAllOpUsesWith(op, op.getOperand());
+    return success();
+  }
+};
+
 struct FoldSignOpPattern : public ShapeOpRewritePattern<SignOp> {
   using ShapeOpRewritePattern::ShapeOpRewritePattern;
 
@@ -1388,6 +1441,7 @@
   patterns->add<FoldRemOpPattern>(context, options, benefit);
   patterns->add<FoldReshapeOpPattern>(context, options, benefit);
   patterns->add<FoldSelectOpPattern>(context, options, benefit);
+  patterns->add<FoldSetDimensionSizeOpPattern>(context, options, benefit);
   patterns->add<FoldSignOpPattern>(context, options, benefit);
   patterns->add<FoldSliceOpPattern>(context, options, benefit);
   patterns->add<FoldSubtractOpPattern>(context, options, benefit);

